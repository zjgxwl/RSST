# ViT-Small 70%剪枝实验 - 参数详细分析

**生成时间**: 2026-01-17 19:50  
**实验脚本**: `run_vit_small_70p_all.sh`

---

## 📋 4个实验概览

| 实验 | 数据集 | 方法 | GPU | 进程ID |
|------|--------|------|-----|--------|
| 实验1 | CIFAR-10 | Refill | GPU 0 | 2116622 |
| 实验2 | CIFAR-10 | RSST | GPU 0 | 2116623 |
| 实验3 | CIFAR-100 | Refill | GPU 1 | 2116624 |
| 实验4 | CIFAR-100 | RSST | GPU 1 | 2116625 |

---

## 🔧 完整启动命令示例（实验1）

```bash
CUDA_VISIBLE_DEVICES=0 conda run -n structlth python -u main_imp_fillback.py \
    --arch vit_small \
    --dataset cifar10 \
    --data data/cifar10 \
    --struct refill \
    --vit_pretrained \
    --vit_structured \
    --vit_prune_target both \
    --criteria magnitude \
    --rate 0.7 \
    --mlp_prune_ratio 0.7 \
    --pruning_times 16 \
    --epochs 60 \
    --batch_size 128 \
    --sorting_mode global \
    --lr 0.01 \
    --fillback_rate 0.0 \
    --init init_model/vit_small_cifar10_pretrained_init.pth.tar \
    --save_dir checkpoint/vit_small_70p/cifar10_refill \
    --exp_name cifar10_refill_70p_0117_1948
```

---

## 📊 参数详细分析

### 1. 环境与运行参数

#### `CUDA_VISIBLE_DEVICES=0`
- **作用**: 指定使用的GPU
- **值**: 
  - 实验1/2: `0` (GPU 0)
  - 实验3/4: `1` (GPU 1)
- **说明**: 控制每个实验使用哪张显卡，实现双GPU并行加速

#### `conda run -n structlth`
- **作用**: 在指定conda环境中运行
- **环境**: `structlth`
- **说明**: 确保使用正确的Python和依赖库

#### `python -u`
- **作用**: `-u` 参数使Python以无缓冲模式运行
- **说明**: 实时输出日志，不等待缓冲区满

---

### 2. 模型与数据集参数

#### `--arch vit_small`
- **参数**: 模型架构
- **值**: `vit_small`
- **说明**: 使用ViT-Small模型
- **配置**:
  - Embed dim: 384
  - Depth: 12层Transformer
  - Num heads: 6个注意力头
  - 总参数: ~21.4M (CIFAR-10) / ~21.3M (CIFAR-100)

#### `--dataset cifar10` / `cifar100`
- **参数**: 数据集名称
- **值**: 
  - 实验1/2: `cifar10` (10个类别)
  - 实验3/4: `cifar100` (100个类别)
- **说明**: 决定分类头的输出维度和数据加载方式

#### `--data data/cifar10` / `data/cifar100`
- **参数**: 数据集路径
- **值**: 数据所在的目录
- **说明**: 指定CIFAR数据集的存储位置

---

### 3. 预训练相关参数

#### `--vit_pretrained`
- **参数**: 是否使用预训练模型
- **类型**: Flag (存在即为True)
- **作用**: ✅ 启用ImageNet预训练权重初始化
- **效果**:
  - 从timm加载 `vit_small_patch16_224` 预训练权重
  - 迁移148个参数（Transformer blocks）
  - 提升初始精度和收敛速度

#### `--init init_model/vit_small_cifar10_pretrained_init.pth.tar`
- **参数**: 初始化权重文件
- **值**: 
  - 实验1/2: `init_model/vit_small_cifar10_pretrained_init.pth.tar`
  - 实验3/4: `init_model/vit_small_cifar100_pretrained_init.pth.tar`
- **验证**: 权重std=0.062 > 0.05 ✅ (确认是预训练模型)
- **说明**: 保存的预训练初始化文件，用于每次剪枝后的权重重置

---

### 4. 剪枝方法参数

#### `--struct refill` / `rsst`
- **参数**: 结构化剪枝方法
- **值**: 
  - 实验1/3: `refill` (Refill方法)
  - 实验2/4: `rsst` (RSST方法)
- **说明**: 
  - **Refill**: 剪枝后不回填，标准IMP方法
  - **RSST**: 剪枝后使用正则化策略训练，本项目的核心方法

#### `--vit_structured`
- **参数**: 启用ViT结构化剪枝
- **类型**: Flag
- **作用**: 对ViT进行结构化剪枝（head/neuron级别），而非element-wise
- **说明**: 剪枝完整的attention head或MLP neurons，保持结构完整性

#### `--vit_prune_target both`
- **参数**: ViT剪枝目标
- **值**: `both` (同时剪枝attention head和MLP)
- **可选**: `head` (仅head), `mlp` (仅MLP), `both` (两者都剪)
- **说明**: 指定要剪枝的ViT组件类型

---

### 5. 剪枝率参数

#### `--rate 0.7`
- **参数**: 总体剪枝率
- **值**: `0.7` (70%)
- **说明**: 
  - 每次迭代剪除70%的权重/结构
  - 对于结构化剪枝，表示剪除70%的heads
  - **剩余**: 30%的heads保留

#### `--mlp_prune_ratio 0.7`
- **参数**: MLP剪枝率
- **值**: `0.7` (70%)
- **说明**: 
  - MLP层的剪枝率（与head剪枝率独立）
  - 剪除70%的MLP neurons
  - 确保head和MLP的剪枝率一致

---

### 6. 训练与迭代参数

#### `--pruning_times 16`
- **参数**: 剪枝迭代次数
- **值**: `16`次
- **说明**: 
  - IMP方法会进行16次剪枝-训练-重置循环
  - 每次迭代剪枝更多的结构
  - 最终达到目标稀疏度

#### `--epochs 60`
- **参数**: 每次迭代的训练轮数
- **值**: `60` epochs
- **说明**: 
  - 每个pruning state训练60个epoch
  - 总训练轮数 = 60 × 16 = 960 epochs
  - 预计每个state耗时 ~72分钟

#### `--batch_size 128`
- **参数**: 批次大小
- **值**: `128`
- **说明**: 
  - 每个batch包含128张图片
  - 适配A800 80GB显存
  - 影响训练速度和模型收敛

---

### 7. 学习率与优化参数

#### `--lr 0.01`
- **参数**: 初始学习率
- **值**: `0.01`
- **说明**: 
  - SGD优化器的起始学习率
  - 会使用learning rate schedule进行衰减
  - 默认策略: MultiStepLR (在91和136 epoch衰减)

#### 其他优化参数（代码默认）
- **Momentum**: `0.9`
- **Weight Decay**: `1e-4`
- **Optimizer**: SGD
- **LR Schedule**: MultiStepLR (gamma=0.1)

---

### 8. 剪枝策略参数

#### `--criteria magnitude`
- **参数**: 剪枝标准
- **值**: `magnitude` (幅度)
- **说明**: 
  - 基于权重/分数的大小进行剪枝
  - 对于结构化剪枝，按head/neuron的重要性分数排序
  - 剪除分数最小的结构

#### `--sorting_mode global`
- **参数**: 排序模式
- **值**: `global` (全局排序)
- **可选**: 
  - `layer-wise`: 每层独立排序剪枝
  - `global`: 所有层混合排序，统一剪枝
- **说明**: 
  - Global模式更激进，能找到全局最不重要的结构
  - 可能导致某些层剪枝更多，某些层保留更多

---

### 9. RSST方法专用参数（仅实验2和4）

#### `--reg_granularity_prune 1.0`
- **参数**: 正则化强度阈值
- **值**: `1.0`
- **作用**: RSST方法的核心参数，控制正则化的强度
- **说明**: 
  - 值越大，正则化越强
  - 影响模型在剪枝后的恢复能力

#### `--RST_schedule exp_custom_exponents`
- **参数**: 正则化调度策略
- **值**: `exp_custom_exponents` (自定义指数调度)
- **可选**: `x`, `x^2`, `x^3`, `exp`, `exp_custom`
- **说明**: 
  - 控制正则化强度随训练进行的变化曲线
  - 指数策略: 开始强，逐渐减弱

#### `--exponents 4`
- **参数**: 指数曲率
- **值**: `4`
- **作用**: 控制指数调度的陡峭程度
- **说明**: 
  - 值越大，曲线越陡峭
  - 影响正则化衰减的速度

---

### 10. Refill方法专用参数（仅实验1和3）

#### `--fillback_rate 0.0`
- **参数**: 回填率
- **值**: `0.0` (不回填)
- **说明**: 
  - 0表示不进行权重回填
  - 相当于标准的IMP方法
  - 如果>0，会在剪枝后回填部分权重

---

### 11. 输出与日志参数

#### `--save_dir checkpoint/vit_small_70p/cifar10_refill`
- **参数**: 模型保存目录
- **值**: 
  - 实验1: `checkpoint/vit_small_70p/cifar10_refill`
  - 实验2: `checkpoint/vit_small_70p/cifar10_rsst`
  - 实验3: `checkpoint/vit_small_70p/cifar100_refill`
  - 实验4: `checkpoint/vit_small_70p/cifar100_rsst`
- **说明**: 保存训练过程中的checkpoint和最佳模型

#### `--exp_name cifar10_refill_70p_0117_1948`
- **参数**: 实验名称
- **值**: 包含数据集、方法、剪枝率、时间戳
- **说明**: 
  - 用于区分不同实验
  - 会用作WandB实验名（如果启用）
  - 便于后期分析和比较

---

## 📊 参数对比：Refill vs RSST

| 参数 | Refill (实验1/3) | RSST (实验2/4) | 区别 |
|------|-----------------|----------------|------|
| `--struct` | `refill` | `rsst` | 方法类型 |
| `--fillback_rate` | `0.0` | - | Refill不回填 |
| `--reg_granularity_prune` | - | `1.0` | RSST正则化强度 |
| `--RST_schedule` | - | `exp_custom_exponents` | RSST调度策略 |
| `--exponents` | - | `4` | RSST曲率参数 |

**其他参数完全相同**，确保公平对比！

---

## 🎯 关键参数总结

### ✅ 最重要的参数

1. **`--vit_pretrained`** ⭐⭐⭐
   - 启用预训练模型，大幅提升效果

2. **`--rate 0.7` + `--mlp_prune_ratio 0.7`** ⭐⭐⭐
   - 70%的高剪枝率，挑战性测试

3. **`--vit_structured` + `--vit_prune_target both`** ⭐⭐⭐
   - 结构化剪枝，实际可用的压缩方法

4. **`--sorting_mode global`** ⭐⭐
   - 全局排序，更激进的剪枝策略

5. **RSST参数** ⭐⭐
   - `--reg_granularity_prune 1.0`
   - `--RST_schedule exp_custom_exponents`
   - `--exponents 4`
   - 核心创新点，与Refill对比

---

## 📈 预期实验结果

### 时间预估
- **单个实验**: ~19.2小时 (16 states × 72分钟)
- **双GPU并行**: ~9-11小时 ⚡

### 精度预期（参考）
| 实验 | 初始精度 | 剪枝后精度 | 说明 |
|------|---------|-----------|------|
| CIFAR-10 Refill | >50% | >85% | 预训练基线 |
| CIFAR-10 RSST | >50% | >87% | RSST优势 |
| CIFAR-100 Refill | >30% | >60% | 预训练基线 |
| CIFAR-100 RSST | >30% | >65% | RSST优势 |

*注: 以上为经验参考值，实际结果取决于训练过程*

---

## 🔍 监控建议

### 关键指标
1. **初始验证精度** (第1个epoch)
   - 应该 >50% (CIFAR-10) 或 >30% (CIFAR-100)
   - 验证预训练模型生效

2. **每个State的最终精度**
   - 应该逐渐恢复到接近初始水平

3. **GPU利用率**
   - 应该保持在80-100%

4. **训练时间**
   - 每个epoch ~1.2分钟
   - 每个state ~72分钟

### 监控命令
```bash
# GPU使用
nvidia-smi

# 查看进度
grep "pruning state" logs_vit_small_70p/*.log

# 查看精度
grep "Acc@1" logs_vit_small_70p/*.log
```

---

**文档生成时间**: 2026-01-17 19:50  
**参数版本**: ViT-Small 70% Pruning v1.0  
**实验状态**: ✅ 运行中
