# ⚠️ 未剪枝基线准确率偏低原因分析

## 📊 问题描述

**实际结果** vs **预期结果**:

| 数据集 | 实际基线 (State 0) | ViT-Small标准 | 差距 |
|--------|-------------------|---------------|------|
| CIFAR-10 | **74.12%** | 95-98% | **-21个百分点** ❌ |
| CIFAR-100 | **51.31%** | 75-80% | **-24个百分点** ❌ |

---

## 🔍 根本原因：模型是从随机初始化开始训练的！

### 证据1: 训练日志显示从零开始

```
Epoch: [0][0/352]  Loss 2.3346  Accuracy 7.031%
Epoch: [0][200/352] Loss 2.1990  Accuracy 17.013%
valid_accuracy 20.660
```

**分析**: 
- 第一个epoch第一个batch的准确率只有**7%** (随机猜测10类应该是10%)
- Loss = 2.3346 ≈ -ln(1/10) = 2.30，这是典型的随机初始化时的交叉熵损失

如果是真正的预训练模型，第一个epoch就应该有**90%+**的准确率！

---

### 证据2: 初始化权重分析

对 `init_model/vit_small_cifar10_pretrained_init.pth.tar` 的分析：

#### Transformer Block权重
```
blocks.0.attn.qkv.weight:
  Mean: 0.000010      ≈ 0  ✓
  Std:  0.020015      ≈ 0.02 (Xavier初始化标准差)  ✓
  Min:  -0.099143
  Max:  0.093967
  → ⚠️ 典型的Xavier随机初始化！
```

#### 分类头权重
```
head.weight (10 x 384):
  Mean: -0.000006     ≈ 0  ✓
  Std:  0.019875      ≈ 0.02  ✓
  
类别0和类别1的余弦相似度: 0.108718 ≈ 0  ✓
  → ⚠️ 如果是训练过的模型，不同类的权重向量应该有明显区分！
```

**结论**: 所有权重都符合**Xavier/Kaiming随机初始化**的特征，**不是训练过的模型**！

---

## 📋 配置检查

### 当前训练配置

```bash
INIT_FILE="init_model/vit_small_cifar10_pretrained_init.pth.tar"  # ⚠️ 名字有误导性
EPOCHS="--epochs 60"           # 每个State只训练60 epochs
LR="--lr 0.01"                 # 学习率0.01
BATCH_SIZE="--batch_size 128"
```

### 为什么60 epochs不够？

**ViT从头训练通常需要**:
- CIFAR-10: **200-300 epochs** 才能达到95%+
- CIFAR-100: **300-400 epochs** 才能达到75%+

**我们只训练了60 epochs**，这只是完整训练的**20-30%**！

---

## 🎯 为什么准确率这么低？

### 原因1: 从随机初始化开始 ❌
- 不是真正的预训练模型
- 模型需要从零学习特征表示

### 原因2: 训练epochs严重不足 ❌
- 只训练了60 epochs
- ViT需要200-300+ epochs才能充分收敛

### 原因3: 学习率可能偏小 ⚠️
- lr=0.01可能对从头训练来说偏保守
- ViT从头训练通常需要更aggressive的学习率策略

### 原因4: 缺乏数据增强策略 ❓
- 不清楚是否使用了足够强的数据增强
- ViT对数据增强很敏感

---

## 💡 这如何影响剪枝实验？

### 当前情况
```
State 0 (未剪枝): 74% ← 随机初始化 + 60 epochs
State 1-15 (剪枝): 在低质量基线上进行迭代剪枝
```

### 问题
1. **基线质量差** → 剪枝后很难恢复
2. **天花板效应** → 最高只能达到81%，远低于ViT的潜力
3. **实验公平性** → 无法真正评估剪枝方法对高性能模型的影响

---

## 🔧 解决方案

### 方案1: 使用真正的预训练模型（推荐）

```bash
# 选项A: 使用ImageNet预训练的ViT-Small
# 从timm或torch hub下载，然后fine-tune到CIFAR

# 选项B: 在CIFAR上从头训练一个高质量模型
# 训练200-300 epochs，达到95%+准确率，保存作为初始化
```

**预期效果**:
- State 0: 95%+ (CIFAR-10)
- State 15剪枝后: 85-90% (仍然可用)

---

### 方案2: 增加每个State的训练epochs

```bash
EPOCHS="--epochs 100"  # 或更多
```

**预期效果**:
- State 0可能达到85-90%
- 但总训练时间会增加67%

---

### 方案3: 调整学习率策略

```bash
LR="--lr 0.1"  # 更大的初始学习率
# 配合更aggressive的学习率衰减
```

---

### 方案4: 接受现状（用于方法对比）

**如果只是为了对比Refill vs RSST**:
- ✅ 当前实验仍然有效（相对对比）
- ✅ 可以观察到Refill优于RSST的趋势
- ❌ 但无法评估方法在高性能模型上的表现

---

## 📊 对比：理想 vs 当前

| 阶段 | 理想情况 | 当前情况 | 差距 |
|------|----------|----------|------|
| **初始化** | ImageNet预训练 | 随机初始化 | ❌❌ |
| **State 0** | 95-98% (CIFAR-10) | 74% | -21% |
| **State 15** | 85-92% (70%剪枝) | 82%预测 | -3-10% |
| **训练时长** | 快（fine-tune） | 慢（从头训练） | 2-3x |

---

## ✅ 结论

### 核心问题
**"pretrained_init"文件名有误导性** - 实际上是**随机初始化**，不是预训练模型。

### 影响
1. 基线准确率低（74% vs 95%）
2. 剪枝后恢复空间有限
3. 无法充分评估剪枝方法的潜力

### 建议
- **短期**: 当前实验继续运行（可用于方法对比）
- **中期**: 训练或获取真正的高质量预训练模型
- **长期**: 建立标准的初始化流程，确保使用高质量基线

---

## 📌 后续实验改进

1. ✅ 获取或训练真正的预训练ViT-Small (95%+ on CIFAR-10)
2. ✅ 增加训练epochs到150-200
3. ✅ 调整学习率策略（更大初始lr + cosine decay）
4. ✅ 加强数据增强（AutoAugment, Mixup, CutMix）
5. ✅ 使用更标准的训练recipe

**预期效果**: 
- State 0: 95-98% (CIFAR-10)
- State 15剪枝后: 88-93% (仍然是SOTA级别)

---

**分析时间**: 2026-01-16 22:00  
**关键发现**: 模型从随机初始化开始训练，而非真正的预训练模型

