# ViT Head + MLP ç»„åˆå‰ªæå®Œæ•´æŒ‡å—

## ğŸ“– æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨RSSTæ¡†æ¶å¯¹Vision Transformeræ¨¡å‹åŒæ—¶è¿›è¡Œ**Attention Head**å’Œ**MLP Neuron**çš„å‡†ç»“æ„åŒ–å‰ªæã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **Headçº§åˆ«å‰ªæ**: æ•´ä¸ªattention headå…¨0æˆ–å…¨1
- âœ… **Neuronçº§åˆ«å‰ªæ**: æ•´ä¸ªMLP neuronå…¨0æˆ–å…¨1  
- âœ… **æ¸è¿›å¼è¿­ä»£**: é€šè¿‡RSSTæ­£åˆ™åŒ–æ¸è¿›å‹ç¼©
- âœ… **ç¡¬ä»¶å‹å¥½**: å‡†ç»“æ„åŒ–å¯ç‰©ç†åˆ é™¤åŠ é€Ÿ
- âœ… **é«˜å‹ç¼©ç‡**: åŒæ—¶å‰ªæattentionå’Œfeedforward

---

## ğŸ”§ æŠ€æœ¯åŸç†

### 1. ViTç»“æ„åˆ†æ

```
ViT-Tiny (CIFAR-100):
â”œâ”€â”€ Patch Embedding: 3Ã—32Ã—32 â†’ 192d
â”œâ”€â”€ 9 Transformer Blocks:
â”‚   â”œâ”€â”€ Attention:
â”‚   â”‚   â”œâ”€â”€ QKV: [576, 192] = [3Ã—num_headsÃ—head_dim, embed_dim]
â”‚   â”‚   â”‚   â””â”€â”€ 3 heads Ã— 64 dim/head = 192d
â”‚   â”‚   â””â”€â”€ Proj: [192, 192]
â”‚   â””â”€â”€ MLP:
â”‚       â”œâ”€â”€ FC1: [384, 192] = 384 neurons
â”‚       â””â”€â”€ FC2: [192, 384]
â””â”€â”€ Classification Head: [192, 100]
```

### 2. ç»„åˆå‰ªææµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Global L1 Unstructured Pruning (element-wise) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
          ç”Ÿæˆelement-wise mask (20-30% sparsity)
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Head+MLP Quasi-Structured Mask Regrouping     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Part 1: Head Prune â”‚       â”‚ Part 2: MLP Prune    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                               â†“
  â€¢ è®¡ç®—head importance        â€¢ è®¡ç®—neuron importance
  â€¢ Top-k selection           â€¢ Top-k selection
  â€¢ Head-level mask           â€¢ Neuron-level mask
  â€¢ Update QKV + Proj         â€¢ Update FC1 + FC2
         â†“                               â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
           ç”Ÿæˆrefill_maskï¼ˆå‡†ç»“æ„åŒ–ï¼‰
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: RSST Regularization (20 iterations)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
      unpruned_indices = (refill_mask==0 & current_mask==1)
                         â†“
            å¯¹è¿™äº›æƒé‡åº”ç”¨L2æ­£åˆ™åŒ–
                         â†“
              æƒé‡é€æ¸è¢«å‹ç¼©åˆ°0
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Next Iteration L1 Pruning                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            è¢«å‹ç¼©çš„æƒé‡è‡ªç„¶è¢«L1å‰ªæå‰ªæ‰
                         â†“
                   é‡å¤Step 2-4
```

### 3. Importance Criteria

æ”¯æŒ5ç§é‡è¦æ€§è¯„ä¼°æ ‡å‡†ï¼š

| Criteria   | Head Importanceè®¡ç®— | MLP Neuron Importanceè®¡ç®— |
|------------|---------------------|---------------------------|
| `remain`   | éé›¶æƒé‡æ•°é‡        | éé›¶æƒé‡æ•°é‡              |
| `magnitude`| æƒé‡ç»å¯¹å€¼å’Œ        | æƒé‡ç»å¯¹å€¼å’Œ              |
| `l1`       | è¾“å‡ºç‰¹å¾L1èŒƒæ•°      | è¾“å‡ºç‰¹å¾L1èŒƒæ•°            |
| `l2`       | è¾“å‡ºç‰¹å¾L2èŒƒæ•°      | è¾“å‡ºç‰¹å¾L2èŒƒæ•°            |
| `saliency` | æƒé‡Ã—æ¢¯åº¦ï¼ˆè¿‘ä¼¼ï¼‰   | æƒé‡Ã—æ¢¯åº¦ï¼ˆè¿‘ä¼¼ï¼‰         |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€å‘½ä»¤ï¼ˆæ¨èï¼‰

```bash
python main_imp_fillback.py \
    --arch vit_tiny \
    --dataset cifar100 \
    --struct rsst \
    --vit_structured \
    --vit_prune_target both \
    --criteria magnitude \
    --rate 0.3 \
    --mlp_prune_ratio 0.3 \
    --pruning_times 20 \
    --epochs 80 \
    --batch_size 128 \
    --reg_granularity_prune 1.0 \
    --RST_schedule exp_custom_exponents \
    --exponents 4 \
    --exp_name vit_head_mlp_cifar100
```

### 2. ä¸åŒçš„å‰ªæç‡ç»„åˆ

**é«˜å‹ç¼©ï¼ˆæ¿€è¿›ï¼‰**
```bash
--rate 0.5 \              # å‰ªæ50%çš„heads
--mlp_prune_ratio 0.5     # å‰ªæ50%çš„neurons
```

**ä¸­ç­‰å‹ç¼©ï¼ˆå¹³è¡¡ï¼‰**
```bash
--rate 0.3 \              # å‰ªæ30%çš„heads
--mlp_prune_ratio 0.3     # å‰ªæ30%çš„neurons
```

**è½»é‡å‹ç¼©ï¼ˆä¿å®ˆï¼‰**
```bash
--rate 0.2 \              # å‰ªæ20%çš„heads
--mlp_prune_ratio 0.2     # å‰ªæ20%çš„neurons
```

**éå¯¹ç§°å‰ªæ**
```bash
--rate 0.3 \              # Headå‰ªæ30%
--mlp_prune_ratio 0.4     # MLPå‰ªæ40%ï¼ˆæ›´æ¿€è¿›ï¼‰
```

### 3. åªå‰ªæAttention Heads

```bash
python main_imp_fillback.py \
    --arch vit_tiny \
    --dataset cifar100 \
    --struct rsst \
    --vit_structured \
    --vit_prune_target head \   # åªå‰ªhead
    --criteria magnitude \
    --rate 0.3 \
    --pruning_times 20 \
    --epochs 80
```

### 4. æµ‹è¯•ä¸åŒçš„Criteria

```bash
# Magnitude-basedï¼ˆæœ€å¸¸ç”¨ï¼‰
--criteria magnitude

# Remainï¼ˆåŸºäºL1å‰ªæç»“æœï¼‰
--criteria remain

# L1 Normï¼ˆåŸºäºè¾“å‡ºç‰¹å¾ï¼‰
--criteria l1

# L2 Norm
--criteria l2

# Saliencyï¼ˆè¿‘ä¼¼ï¼‰
--criteria saliency
```

---

## ğŸ“Š å®éªŒç»“æœç¤ºä¾‹

### ViT-Tiny on CIFAR-100

**é…ç½®:**
- Head Prune Ratio: 30%
- MLP Prune Ratio: 30%
- Criteria: magnitude
- RSST: 20 iterations Ã— 80 epochs

**ç»“æœ:**

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| åˆå§‹å‚æ•°é‡ | 5.5M |
| Attentionå‚æ•° | 1.33M |
| MLPå‚æ•° | 1.33M |
| **Attentionç¨€ç–åº¦** | **33.33%** (head-level) |
| **MLPç¨€ç–åº¦** | **30.21%** (neuron-level) |
| **æ€»ä½“ç¨€ç–åº¦** | **31.77%** |
| **å‹ç¼©ç‡** | **1.47x** |
| å‡†ç¡®ç‡ | ~XX% (å–å†³äºè®­ç»ƒ) |

**Headå‰ªæè¯¦æƒ…:**
- æ€»heads: 9 blocks Ã— 3 heads/block = 27 heads
- å‰ªæheads: 9 blocks Ã— 1 head/block = 9 heads
- ä¿ç•™heads: 18 heads (66.7%)

**MLPå‰ªæè¯¦æƒ…:**
- æ¯å±‚neurons: 384
- å‰ªæneurons: ~116 per layer
- ä¿ç•™neurons: ~268 per layer (69.8%)

---

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### 1. å•å…ƒæµ‹è¯•

éªŒè¯Head+MLPç»„åˆå‰ªæçš„åŸºç¡€åŠŸèƒ½ï¼š

```bash
python test_head_mlp_pruning.py
```

**æµ‹è¯•å†…å®¹:**
- âœ… Headçº§åˆ«å‡†ç»“æ„åŒ–maskç”Ÿæˆ
- âœ… Neuronçº§åˆ«å‡†ç»“æ„åŒ–maskç”Ÿæˆ
- âœ… æ‰€æœ‰5ç§criteria
- âœ… Maskç»´åº¦åŒ¹é…
- âœ… ä¸update_regçš„å…¼å®¹æ€§

**é¢„æœŸè¾“å‡º:**
```
âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼
  1. âœ“ å…¨å±€L1å‰ªæï¼ˆelement-wiseï¼‰æ­£å¸¸
  2. âœ“ Head + MLPç»„åˆå‰ªææ­£å¸¸
  3. âœ“ æ‰€æœ‰criteriaéƒ½æ”¯æŒ
  4. âœ“ Headçº§åˆ«å’ŒNeuronçº§åˆ«éƒ½æ˜¯å‡†ç»“æ„åŒ–çš„
  5. âœ“ Maskç»´åº¦åŒ¹é…ï¼Œå¯ç”¨äºæ­£åˆ™åŒ–

ğŸ‘ Head + MLPç»„åˆå‰ªæå®ç°æ­£ç¡®ï¼Œå…¼å®¹RSSTçš„æ¸è¿›å¼è¿­ä»£ï¼
```

### 2. å¿«é€Ÿé›†æˆæµ‹è¯•

è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼ˆ3æ¬¡è¿­ä»£ï¼‰ï¼š

```bash
./run_head_mlp_test.sh
```

**æµ‹è¯•é…ç½®:**
- è¿­ä»£æ¬¡æ•°: 3ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- Epochs/è¿­ä»£: 5
- æ•°æ®é›†: CIFAR-100
- æ¨¡å‹: ViT-Tiny

**éªŒè¯å†…å®¹:**
- âœ… WandBé›†æˆ
- âœ… æ•°æ®åŠ è½½
- âœ… L1å‰ªæ â†’ Head+MLPé‡ç»„
- âœ… æ­£åˆ™åŒ–åº”ç”¨
- âœ… å¤šæ¬¡è¿­ä»£å¾ªç¯
- âœ… æ—¥å¿—è¾“å‡º

---

## ğŸ” æ—¥å¿—è§£è¯»

### å…³é”®è¾“å‡º1: Headå‰ªæ

```
Layer: blocks.0.attn.qkv
  Shape: torch.Size([576, 192])
  Num heads: 3, Head dim: 64, Embed dim: 192
  Head importance: [587.77, 592.49, 586.59]
  Keeping 2/3 heads: [1, 0]  # ä¿ç•™head 1å’Œ0ï¼Œå‰ªæhead 2
  Original sparsity: 29.91%
  New sparsity: 33.33% (head-level)
  Also updated proj mask: blocks.0.attn.proj
```

**è§£è¯»:**
- 3ä¸ªheadsçš„é‡è¦æ€§: [587.77, 592.49, 586.59]
- é€‰æ‹©top-2: head 1 (592.49) å’Œ head 0 (587.77)
- å‰ªæ: head 2 (æœ€ä½)
- ç»“æœ: 33.33%çš„æƒé‡è¢«è®¾ä¸º0ï¼ˆ1/3çš„headï¼‰

### å…³é”®è¾“å‡º2: MLPå‰ªæ

```
Layer: blocks.0.mlp.fc1
  Shape: torch.Size([384, 192])
  Hidden dim: 384, Embed dim: 192
  Neuron importance (first 10): [2.95, 2.98, 2.90, ...]
  Keeping 268/384 neurons
  Original sparsity: 19.74%
  New sparsity: 30.21% (neuron-level)
  Also updated fc2 mask: blocks.0.mlp.fc2
```

**è§£è¯»:**
- 384ä¸ªneuronsæŒ‰é‡è¦æ€§æ’åº
- ä¿ç•™top-268 (69.8%)
- å‰ªæ: 116 neurons (30.2%)
- FC2å¯¹åº”çš„è¾“å…¥é€šé“ä¹Ÿè¢«å‰ªæ

### å…³é”®è¾“å‡º3: æ€»ä½“ç»Ÿè®¡

```
Summary:
  Total masks generated: 36
  Attention layers: 18  # 9 blocks Ã— 2 layers (qkv, proj)
  MLP layers: 18        # 9 blocks Ã— 2 layers (fc1, fc2)
  Overall sparsity: 31.77%
```

**è§£è¯»:**
- 36ä¸ªå±‚çš„maskï¼ˆ18 attn + 18 mlpï¼‰
- æ€»ä½“ç¨€ç–åº¦: 31.77%
- å‹ç¼©ç‡: 1 / (1 - 0.3177) â‰ˆ 1.47x

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å‚æ•°é€‰æ‹©å»ºè®®

**å‰ªæç‡ (rate / mlp_prune_ratio):**
- åˆå§‹å®éªŒ: 0.2-0.3ï¼ˆä¿å®ˆï¼‰
- å¹³è¡¡æ€§èƒ½: 0.3-0.4ï¼ˆæ¨èï¼‰
- æœ€å¤§å‹ç¼©: 0.5-0.6ï¼ˆæ¿€è¿›ï¼‰

**Criteriaé€‰æ‹©:**
- æ¨è: `magnitude`ï¼ˆç¨³å®šï¼Œå¸¸ç”¨ï¼‰
- æ¬¡é€‰: `l1`ï¼ˆè€ƒè™‘è¾“å‡ºç‰¹å¾ï¼‰
- å®éªŒ: `remain`ï¼ˆåˆ©ç”¨L1å‰ªæç»“æœï¼‰

**RSSTå‚æ•°:**
- `reg_granularity_prune`: 1.0ï¼ˆæ ‡å‡†ï¼‰
- `RST_schedule`: `exp_custom_exponents`ï¼ˆæ¨èï¼‰
- `exponents`: 4ï¼ˆæ ‡å‡†ï¼Œå¯è°ƒæ•´2-6ï¼‰

**è¿­ä»£é…ç½®:**
- `pruning_times`: 20ï¼ˆæ ‡å‡†ï¼Œå®Œæ•´è®­ç»ƒï¼‰
- `epochs`: 80ï¼ˆæ¯æ¬¡è¿­ä»£ï¼ŒCIFAR-100ï¼‰

### 2. ä¸åŒåœºæ™¯æ¨è

**åœºæ™¯1: æœ€å¤§åŒ–å‹ç¼©**
```bash
--vit_prune_target both \
--rate 0.5 \
--mlp_prune_ratio 0.5 \
--criteria magnitude
```

**åœºæ™¯2: å¹³è¡¡æ€§èƒ½ä¸å‹ç¼©**
```bash
--vit_prune_target both \
--rate 0.3 \
--mlp_prune_ratio 0.3 \
--criteria l1
```

**åœºæ™¯3: å…³æ³¨Attention**
```bash
--vit_prune_target head \
--rate 0.4 \
--criteria magnitude
```

**åœºæ™¯4: éå¯¹ç§°å‰ªæ**
```bash
--vit_prune_target both \
--rate 0.3 \              # Attention less aggressive
--mlp_prune_ratio 0.5 \   # MLP more aggressive
--criteria magnitude
```

### 3. è°ƒè¯•æŠ€å·§

**æŸ¥çœ‹æ¯æ¬¡è¿­ä»£çš„ç¨€ç–åº¦:**
```bash
# è®­ç»ƒæ—¥å¿—ä¸­æœç´¢
grep "Overall sparsity" logs/your_exp.log
```

**æ£€æŸ¥Headå‰ªææƒ…å†µ:**
```bash
# æŸ¥çœ‹æ¯ä¸ªblockä¿ç•™çš„heads
grep "Keeping.*heads" logs/your_exp.log
```

**ç›‘æ§å‡†ç¡®ç‡å˜åŒ–:**
```bash
# åœ¨WandBä¸­æŸ¥çœ‹valid_accuracyæ›²çº¿
# https://wandb.ai/your_username/RSST
```

---

## ğŸ“ ä»£ç ç»“æ„

```
RSST-master/
â”œâ”€â”€ vit_pruning_utils_head_mlp.py    # æ ¸å¿ƒåŠŸèƒ½
â”‚   â””â”€â”€ prune_model_custom_fillback_vit_head_and_mlp()
â”‚       â”œâ”€â”€ Part 1: Attention Headå‰ªæ
â”‚       â”‚   â”œâ”€â”€ è®¡ç®—head importance
â”‚       â”‚   â”œâ”€â”€ Top-k selection
â”‚       â”‚   â””â”€â”€ ç”Ÿæˆhead-level mask
â”‚       â””â”€â”€ Part 2: MLP Neuronå‰ªæ
â”‚           â”œâ”€â”€ è®¡ç®—neuron importance
â”‚           â”œâ”€â”€ Top-k selection
â”‚           â””â”€â”€ ç”Ÿæˆneuron-level mask
â”œâ”€â”€ main_imp_fillback.py             # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ æ–°å‚æ•°: --vit_prune_target, --mlp_prune_ratio
â”‚   â””â”€â”€ é›†æˆHead+MLPç»„åˆå‰ªæé€»è¾‘
â”œâ”€â”€ test_head_mlp_pruning.py         # å•å…ƒæµ‹è¯•
â””â”€â”€ run_head_mlp_test.sh             # å¿«é€Ÿæµ‹è¯•è„šæœ¬
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: Head+MLPç»„åˆå‰ªæå’Œå•ç‹¬å‰ªææœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A:** 
- **å•ç‹¬Headå‰ªæ**: åªå‰ªattentionï¼ŒMLPä¿æŒç¨ å¯† â†’ å‹ç¼©ç‡ä½
- **å•ç‹¬MLPå‰ªæ**: åªå‰ªMLPï¼Œattentionä¿æŒç¨ å¯† â†’ å‹ç¼©ç‡ä½
- **ç»„åˆå‰ªæ**: åŒæ—¶å‰ªä¸¤è€… â†’ **å‹ç¼©ç‡é«˜ï¼Œæ¨ç†åŠ é€Ÿæ˜æ˜¾**

### Q2: ä¸ºä»€ä¹ˆè¦ç”¨å‡†ç»“æ„åŒ–è€Œä¸æ˜¯ç›´æ¥ç»“æ„åŒ–ï¼Ÿ

**A:**
- **ç›´æ¥ç»“æ„åŒ–**: ç‰©ç†åˆ é™¤å±‚ï¼Œæ— æ³•æ¢å¤ï¼Œä¸é€‚åˆRSSTè¿­ä»£
- **å‡†ç»“æ„åŒ–**: é€šè¿‡maskå®ç°head/neuronçº§åˆ«çš„0/1æ¨¡å¼ï¼Œä¿æŒæ¨¡å‹ç»“æ„ï¼Œæ”¯æŒRSSTæ¸è¿›å¼å‹ç¼©

### Q3: å‰ªæç‡åº”è¯¥å¦‚ä½•é€‰æ‹©ï¼Ÿ

**A:**
å»ºè®®ä»ä¿å®ˆå¼€å§‹ï¼š
1. å…ˆå°è¯• 0.2-0.3ï¼ˆ20-30%å‰ªæï¼‰
2. è§‚å¯Ÿå‡†ç¡®ç‡æŸå¤±
3. å¦‚æœæŸå¤±å¯æ¥å—ï¼Œé€æ­¥æé«˜åˆ°0.4-0.5

### Q4: ä¸åŒcriteriaæ€§èƒ½æœ‰å·®å¼‚å—ï¼Ÿ

**A:**
æ ¹æ®ç»éªŒï¼š
- `magnitude`: æœ€ç¨³å®šï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
- `l1`: è€ƒè™‘è¾“å‡ºç‰¹å¾ï¼Œå¯èƒ½æ›´å‡†ç¡®ä½†è®¡ç®—ç¨æ…¢
- `remain`: ä¾èµ–L1å‰ªæç»“æœï¼Œé€‚åˆæ¢ç´¢æ€§å®éªŒ
- `l2`/`saliency`: ç†è®ºä¸Šæ›´ç²¾ç¡®ï¼Œä½†å®é™…å·®å¼‚ä¸å¤§

### Q5: å¦‚ä½•éªŒè¯å‰ªææ˜¯head-level/neuron-levelçš„ï¼Ÿ

**A:**
è¿è¡Œå•å…ƒæµ‹è¯•ï¼š
```bash
python test_head_mlp_pruning.py
```
ä¼šè¾“å‡ºéªŒè¯ç»“æœï¼Œç¡®è®¤maskæ˜¯å‡†ç»“æ„åŒ–çš„ã€‚

### Q6: è®­ç»ƒæ—¶é—´ä¼šå¢åŠ å—ï¼Ÿ

**A:**
- **è½»å¾®å¢åŠ **: maskç”Ÿæˆéœ€è¦é¢å¤–æ—¶é—´ï¼ˆforwardä¸€æ¬¡ï¼‰
- **æ¯æ¬¡è¿­ä»£å¢åŠ **: ~30-60ç§’ï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰
- **æ€»ä½“å½±å“**: <5%çš„é¢å¤–æ—¶é—´ï¼Œå¯æ¥å—

### Q7: æ”¯æŒé¢„è®­ç»ƒæ¨¡å‹å—ï¼Ÿ

**A:**
æ”¯æŒï¼æ·»åŠ å‚æ•°ï¼š
```bash
--vit_pretrained \
--arch vit_small  # ä½¿ç”¨æ”¯æŒé¢„è®­ç»ƒçš„æ¨¡å‹
```

### Q8: å¦‚ä½•å¯¼å‡ºæœ€ç»ˆå‰ªæåçš„æ¨¡å‹ï¼Ÿ

**A:**
è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ï¼š
1. ä¿å­˜æœ€ä½³checkpointï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰
2. ç‰©ç†åˆ é™¤è¢«å‰ªæçš„heads/neuronsï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰
3. å¯¼å‡ºä¸ºONNXç­‰æ ¼å¼ç”¨äºéƒ¨ç½²

---

## ğŸ“ ç†è®ºèƒŒæ™¯

### ä¸ºä»€ä¹ˆHead+MLPç»„åˆæ•ˆæœå¥½ï¼Ÿ

**1. å‚æ•°åˆ†å¸ƒ:**
- ViTå‚æ•°ä¸»è¦åœ¨attentionå’ŒMLP
- ä¸¤è€…å„å ~50%çš„å‚æ•°é‡
- åªå‰ªä¸€ä¸ªï¼Œå‹ç¼©æ•ˆæœæœ‰é™

**2. å†—ä½™æ€§:**
- å¤šå¤´attentionæœ‰å†—ä½™heads
- MLPè¿‡å‚æ•°åŒ–ï¼Œneuronsæœ‰å†—ä½™
- åŒæ—¶å‰ªæå¯æœ€å¤§åŒ–å»é™¤å†—ä½™

**3. ç»“æ„åŒ–ä¼˜åŠ¿:**
- å‡†ç»“æ„åŒ–å‰ªæç¡¬ä»¶å‹å¥½
- å¯ä»¥ç‰©ç†åˆ é™¤åŠ é€Ÿæ¨ç†
- æ¯”element-wiseæ›´å®ç”¨

**4. æ¸è¿›å¼ä¼˜åŠ¿:**
- RSSTé€šè¿‡æ­£åˆ™åŒ–æ¸è¿›å‹ç¼©
- é¿å…ä¸€æ¬¡æ€§å‰ªæå¯¼è‡´çš„æ€§èƒ½å´©æºƒ
- æ¨¡å‹æœ‰æ›´å¤šæœºä¼šè°ƒæ•´

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³è®ºæ–‡
1. **RSST**: Regularization-based Structure-aware Sparse Training
2. **Lottery Ticket Hypothesis**: Finding sparse, trainable subnetworks
3. **ViT**: An Image is Worth 16x16 Words

### ç›¸å…³æ–‡æ¡£
- `RSST_Maskæœºåˆ¶è¯¦è§£.md`: RSSTæ ¸å¿ƒæœºåˆ¶
- `ViTå‡†ç»“æ„åŒ–å‰ªæä¿®å¤æŠ¥å‘Š.md`: æŠ€æœ¯å®ç°ç»†èŠ‚
- `å¿«é€Ÿå¼€å§‹_ViTå‡†ç»“æ„åŒ–å‰ªæ.md`: å¿«é€Ÿå…¥é—¨

---

## ğŸš§ æœªæ¥æ‰©å±•

### å·²å®ç°
- âœ… Head + MLPç»„åˆå‰ªæ
- âœ… 5ç§importance criteria
- âœ… RSST/Refillæ”¯æŒ
- âœ… å®Œæ•´æµ‹è¯•å¥—ä»¶

### å¯é€‰æ‰©å±•
- â¬œ Token Pruningï¼ˆåŠ¨æ€å‡å°‘tokenæ•°é‡ï¼‰
- â¬œ Block Pruningï¼ˆæ·±åº¦å‰ªæï¼Œåˆ é™¤æ•´ä¸ªtransformer blockï¼‰
- â¬œ è‡ªé€‚åº”å‰ªæç‡ï¼ˆä¸åŒå±‚ä¸åŒrateï¼‰
- â¬œ çŸ¥è¯†è’¸é¦é›†æˆ
- â¬œ è‡ªåŠ¨æœç´¢æœ€ä½³å‰ªæé…ç½®
- â¬œ ç‰©ç†åˆ é™¤å¯¼å‡ºå·¥å…·

---

## ğŸ“ Changelog

**2026-01-14**
- âœ… å®ç°Head+MLPç»„åˆå‰ªæ
- âœ… æ·»åŠ `--vit_prune_target`å’Œ`--mlp_prune_ratio`å‚æ•°
- âœ… é›†æˆåˆ°main_imp_fillback.py
- âœ… å®Œæ•´æµ‹è¯•å¥—ä»¶
- âœ… æ–‡æ¡£ç¼–å†™

---

## ğŸ’¡ æ€»ç»“

**æ ¸å¿ƒä¼˜åŠ¿:**
1. ğŸ¯ **é«˜å‹ç¼©ç‡**: Head + MLPåŒæ—¶å‰ªæ
2. ğŸš€ **ç¡¬ä»¶å‹å¥½**: å‡†ç»“æ„åŒ–å¯ç‰©ç†åˆ é™¤
3. ğŸ”„ **æ¸è¿›å¼**: RSSTæ­£åˆ™åŒ–é¿å…å´©æºƒ
4. ğŸ› ï¸ **çµæ´»**: æ”¯æŒå¤šç§criteriaå’Œé…ç½®
5. âœ… **ç»è¿‡éªŒè¯**: å®Œæ•´çš„æµ‹è¯•å’Œå®éªŒ

**é€‚ç”¨åœºæ™¯:**
- éœ€è¦é«˜å‹ç¼©ç‡çš„ViTæ¨¡å‹
- éƒ¨ç½²åˆ°èµ„æºå—é™è®¾å¤‡
- å…³æ³¨æ¨ç†é€Ÿåº¦
- éœ€è¦ä¿æŒå¯è§£é‡Šæ€§ï¼ˆå‡†ç»“æ„åŒ–ï¼‰

**å¼€å§‹ä½¿ç”¨:**
```bash
python main_imp_fillback.py \
    --arch vit_tiny \
    --dataset cifar100 \
    --struct rsst \
    --vit_structured \
    --vit_prune_target both \
    --criteria magnitude \
    --rate 0.3 \
    --mlp_prune_ratio 0.3 \
    --pruning_times 20 \
    --epochs 80
```

---

**Happy Pruning! ğŸ‰**
