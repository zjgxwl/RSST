# Mamba-Small Baseline ä¼˜åŒ–å»ºè®®

**åˆ†ææ—¶é—´**: 2026-01-19  
**å½“å‰æ€§èƒ½**: CIFAR-10 94-95.5%, CIFAR-100 76-81%  
**ä¼˜åŒ–ç›®æ ‡**: CIFAR-10 96%+, CIFAR-100 82-85%+

---

## ğŸ“Š ä¼˜åŒ–ç©ºé—´åˆ†ææ€»è§ˆ

| ä¼˜åŒ–ç±»åˆ« | å½“å‰çŠ¶æ€ | æ½œåœ¨æå‡ | éš¾åº¦ | ä¼˜å…ˆçº§ |
|---------|---------|---------|------|--------|
| **1. æ•°æ®å¢å¼º** | åŸºç¡€ | +1-2% | ä½ | â­â­â­ |
| **2. æ¨¡å‹æ­£åˆ™åŒ–** | éƒ¨åˆ† | +0.5-1% | ä½ | â­â­â­ |
| **3. è®­ç»ƒæŠ€å·§** | åŸºç¡€ | +1-1.5% | ä¸­ | â­â­ |
| **4. æ¨¡å‹æ¶æ„** | æ ‡å‡† | +0.5-1% | é«˜ | â­â­ |
| **5. æ¨ç†ä¼˜åŒ–** | æ—  | +0.5-1% | ä½ | â­â­ |
| **6. å·¥ç¨‹ä¼˜åŒ–** | åŸºç¡€ | 2-3Ã— é€Ÿåº¦ | ä¸­ | â­ |
| **7. Mambaç‰¹å®š** | æ ‡å‡† | +1-2% | é«˜ | â­â­â­ |

---

## ğŸ¯ ä¼˜å…ˆçº§ 1: é«˜æ€§ä»·æ¯”ä¼˜åŒ–ï¼ˆç«‹å³å¯åšï¼‰

### 1.1 â­â­â­ æ·»åŠ  Drop Path (Stochastic Depth)

**é—®é¢˜**: å½“å‰ä»£ç æœ‰ `--drop_path` å‚æ•°ï¼Œä½†**æ¨¡å‹ä¸­æ²¡æœ‰å®é™…ä½¿ç”¨**ï¼

**ä¿®æ”¹ä½ç½®**: `models/mamba.py` çš„ `MambaBlock`

```python
class MambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, 
                 use_mlp=True, mlp_ratio=4.0, dropout=0.0, drop_path=0.0):  # æ·»åŠ  drop_path
        super().__init__()
        self.d_model = d_model
        self.use_mlp = use_mlp
        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()  # æ–°å¢
        
        # ... å…¶ä»–ä»£ç ä¸å˜ ...
    
    def forward(self, x):
        # SSMè·¯å¾„ (with residual + drop_path)
        x = x + self.drop_path(self.ssm(self.norm1(x)))  # ä¿®æ”¹
        
        # MLPè·¯å¾„ (with residual + drop_path)
        if self.use_mlp:
            x = x + self.drop_path(self.mlp(self.norm2(x)))  # ä¿®æ”¹
        
        return x

# æ·»åŠ  DropPath ç±»
class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample"""
    def __init__(self, drop_prob=0.):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0. or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output
```

**é¢„æœŸæå‡**: +0.5-1%  
**æ¨èå€¼**: 0.1-0.2

---

### 1.2 â­â­â­ æ·»åŠ æ›´å¤šæ•°æ®å¢å¼º

**å½“å‰**: åªæœ‰ RandAugment + Mixup + Cutmix

**æ¨èæ·»åŠ **:

#### A. Random Erasing (æ“¦é™¤å¢å¼º)

```python
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                       (0.2023, 0.1994, 0.2010)),
    transforms.RandomErasing(p=0.25),  # æ–°å¢ï¼25% æ¦‚ç‡æ“¦é™¤
])
```

**é¢„æœŸæå‡**: +0.3-0.5%

#### B. AutoAugment (æ¯” RandAugment æ›´å¼º)

```python
from torchvision.transforms import AutoAugment, AutoAugmentPolicy

train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    AutoAugment(policy=AutoAugmentPolicy.CIFAR10),  # æ›¿æ¢ RandAugment
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                       (0.2023, 0.1994, 0.2010)),
    transforms.RandomErasing(p=0.25),
])
```

**é¢„æœŸæå‡**: +0.5-1%ï¼ˆæ›¿ä»£ RandAugmentï¼‰

---

### 1.3 â­â­â­ EMA (Exponential Moving Average)

**åŸç†**: ä½¿ç”¨å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œæå‡æµ‹è¯•æ€§èƒ½

```python
class ModelEMA:
    """æ¨¡å‹å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        # åˆå§‹åŒ– shadow
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        """åº”ç”¨ EMA å‚æ•°ï¼ˆæµ‹è¯•æ—¶ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        """æ¢å¤åŸå§‹å‚æ•°ï¼ˆè®­ç»ƒæ—¶ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}

# ä½¿ç”¨æ–¹æ³•
ema = ModelEMA(model, decay=0.9999)

# è®­ç»ƒå¾ªç¯ä¸­
for batch in train_loader:
    # ... å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ ...
    optimizer.step()
    ema.update()  # æ›´æ–° EMA

# éªŒè¯æ—¶
ema.apply_shadow()  # ä½¿ç”¨ EMA å‚æ•°
val_acc = validate(model, test_loader, criterion, args)
ema.restore()  # æ¢å¤è®­ç»ƒå‚æ•°
```

**é¢„æœŸæå‡**: +0.3-0.7%  
**æ¨èå€¼**: decay=0.9999

---

### 1.4 â­â­ Gradient Clipping

**é—®é¢˜**: è®­ç»ƒå¯èƒ½ä¸ç¨³å®šï¼Œç‰¹åˆ«æ˜¯ SSM å±‚

```python
# åœ¨ optimizer.step() ä¹‹å‰
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

**é¢„æœŸæå‡**: +0.2-0.5%ï¼ˆä¸»è¦æå‡ç¨³å®šæ€§ï¼‰  
**æ¨èå€¼**: max_norm=1.0

---

### 1.5 â­â­ Test-Time Augmentation (TTA)

**åŸç†**: æµ‹è¯•æ—¶ä½¿ç”¨å¤šä¸ªå¢å¼ºç‰ˆæœ¬æŠ•ç¥¨

```python
def validate_with_tta(model, test_loader, criterion, args, n_augment=5):
    """å¸¦ TTA çš„éªŒè¯"""
    model.eval()
    
    all_outputs = []
    all_targets = []
    
    # TTA transforms
    tta_transforms = [
        transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=p),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), 
                               (0.2023, 0.1994, 0.2010)),
        ])
        for p in [0, 0.5, 1.0]  # 3ç§ç¿»è½¬ç­–ç•¥
    ]
    
    with torch.no_grad():
        for images, targets in test_loader:
            batch_outputs = []
            
            # å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨å¤šä¸ªå¢å¼º
            for transform in tta_transforms:
                aug_images = torch.stack([transform(img) for img in images])
                aug_images = aug_images.cuda()
                outputs = model(aug_images)
                batch_outputs.append(outputs)
            
            # å¹³å‡æ‰€æœ‰å¢å¼ºçš„è¾“å‡º
            avg_output = torch.stack(batch_outputs).mean(dim=0)
            all_outputs.append(avg_output)
            all_targets.append(targets)
    
    # è®¡ç®—å‡†ç¡®ç‡
    all_outputs = torch.cat(all_outputs)
    all_targets = torch.cat(all_targets).cuda()
    acc = accuracy(all_outputs, all_targets, topk=(1,))[0]
    
    return acc.item()
```

**é¢„æœŸæå‡**: +0.5-1%  
**æˆæœ¬**: æ¨ç†æ—¶é—´å¢åŠ  3-5Ã—

---

## ğŸš€ ä¼˜å…ˆçº§ 2: è¿›é˜¶ä¼˜åŒ–ï¼ˆä¸­ç­‰éš¾åº¦ï¼‰

### 2.1 â­â­â­ æ··åˆç²¾åº¦è®­ç»ƒ (AMP)

**å¥½å¤„**: 
- è®­ç»ƒé€Ÿåº¦æå‡ **2-3Ã—**
- æ˜¾å­˜èŠ‚çœ **30-40%**
- ç²¾åº¦åŸºæœ¬ä¸å˜ï¼ˆç”šè‡³ç•¥æœ‰æå‡ï¼‰

```python
from torch.cuda.amp import autocast, GradScaler

# åˆå§‹åŒ–
scaler = GradScaler()

# è®­ç»ƒå¾ªç¯
for images, targets in train_loader:
    optimizer.zero_grad()
    
    # ä½¿ç”¨æ··åˆç²¾åº¦
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, targets)
    
    # ç¼©æ”¾æ¢¯åº¦
    scaler.scale(loss).backward()
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    scaler.step(optimizer)
    scaler.update()
```

**é¢„æœŸæå‡**: 
- é€Ÿåº¦: +100-200%
- ç²¾åº¦: +0.0-0.2%

---

### 2.2 â­â­ Layer-wise Learning Rate Decay

**åŸç†**: ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ï¼ˆæµ…å±‚å°ï¼Œæ·±å±‚å¤§ï¼‰

```python
def get_layer_wise_lr_params(model, lr, decay_rate=0.65):
    """
    ä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    æ·±å±‚å­¦ä¹ ç‡å¤§ï¼Œæµ…å±‚å­¦ä¹ ç‡å°
    """
    parameter_groups = []
    
    # Patch embedding (æœ€å°å­¦ä¹ ç‡)
    parameter_groups.append({
        'params': model.patch_embed.parameters(),
        'lr': lr * (decay_rate ** 24)
    })
    
    # Blocks (é€å±‚é€’å¢)
    for i, block in enumerate(model.blocks):
        parameter_groups.append({
            'params': block.parameters(),
            'lr': lr * (decay_rate ** (24 - i))
        })
    
    # Head (æœ€å¤§å­¦ä¹ ç‡)
    parameter_groups.append({
        'params': model.head.parameters(),
        'lr': lr
    })
    
    return parameter_groups

# ä½¿ç”¨
param_groups = get_layer_wise_lr_params(model, lr=1e-3, decay_rate=0.65)
optimizer = optim.AdamW(param_groups, weight_decay=0.05)
```

**é¢„æœŸæå‡**: +0.3-0.5%  
**æ¨è**: decay_rate=0.65-0.75

---

### 2.3 â­â­ æ›´å¥½çš„å­¦ä¹ ç‡ Warmup

**å½“å‰**: çº¿æ€§ warmup  
**æ¨è**: Exponential warmupï¼ˆæ›´å¹³æ»‘ï¼‰

```python
def get_cosine_schedule_with_exp_warmup(optimizer, num_warmup_steps, 
                                         num_training_steps, min_lr=0):
    """
    Exponential warmup + Cosine decay
    """
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Exponential warmup: æ›´å¹³æ»‘
            progress = current_step / num_warmup_steps
            return (1 - np.exp(-5 * progress)) / (1 - np.exp(-5))
        
        # Cosine decay
        progress = (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps)
        return max(min_lr, 0.5 * (1.0 + np.cos(np.pi * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

**é¢„æœŸæå‡**: +0.1-0.3%ï¼ˆä¸»è¦æå‡ç¨³å®šæ€§ï¼‰

---

### 2.4 â­â­ Knowledge Distillation (è‡ªè’¸é¦)

**åŸç†**: ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³ checkpoint ä½œä¸ºæ•™å¸ˆ

```python
# æ­¥éª¤ 1: å…ˆè®­ç»ƒä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ï¼ˆ300 epochsï¼‰
# æ­¥éª¤ 2: ä½¿ç”¨æ•™å¸ˆæ¨¡å‹è’¸é¦å­¦ç”Ÿæ¨¡å‹

def distillation_loss(student_outputs, teacher_outputs, targets, 
                      temperature=4.0, alpha=0.5):
    """
    è’¸é¦æŸå¤± = Î± * KLæ•£åº¦ + (1-Î±) * CEæŸå¤±
    """
    # KL æ•£åº¦
    soft_loss = F.kl_div(
        F.log_softmax(student_outputs / temperature, dim=1),
        F.softmax(teacher_outputs / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature ** 2)
    
    # äº¤å‰ç†µ
    hard_loss = F.cross_entropy(student_outputs, targets)
    
    return alpha * soft_loss + (1 - alpha) * hard_loss

# è®­ç»ƒå¾ªç¯
teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å†»ç»“
student_model.train()

with torch.no_grad():
    teacher_outputs = teacher_model(images)

student_outputs = student_model(images)
loss = distillation_loss(student_outputs, teacher_outputs, targets)
```

**é¢„æœŸæå‡**: +0.5-1%  
**æ¨è**: temperature=4.0, alpha=0.5

---

## ğŸ”¬ ä¼˜å…ˆçº§ 3: Mamba ç‰¹å®šä¼˜åŒ–ï¼ˆé«˜éš¾åº¦ï¼‰

### 3.1 â­â­â­ Bidirectional SSM (åŒå‘æ‰«æ)

**é—®é¢˜**: å½“å‰ SSM æ˜¯å•å‘çš„ï¼Œå›¾åƒä¿¡æ¯å¯èƒ½ä¸¢å¤±

**æ–¹æ¡ˆ**: å®ç°åŒå‘ SSM

```python
class BidirectionalSSM(nn.Module):
    """åŒå‘ SSM"""
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.forward_ssm = SelectiveSSM(d_model, d_state, d_conv, expand)
        self.backward_ssm = SelectiveSSM(d_model, d_state, d_conv, expand)
        self.merge = nn.Linear(d_model * 2, d_model)  # èåˆå‰å‘å’Œåå‘
    
    def forward(self, x):
        # å‰å‘æ‰«æ
        forward_out = self.forward_ssm(x)
        
        # åå‘æ‰«æï¼ˆç¿»è½¬åºåˆ—ï¼‰
        x_reversed = torch.flip(x, dims=[1])  # ç¿»è½¬åºåˆ—ç»´åº¦
        backward_out = self.backward_ssm(x_reversed)
        backward_out = torch.flip(backward_out, dims=[1])  # ç¿»è½¬å›æ¥
        
        # èåˆ
        merged = self.merge(torch.cat([forward_out, backward_out], dim=-1))
        return merged
```

**é¢„æœŸæå‡**: +1-2%  
**æˆæœ¬**: å‚æ•°é‡ +100%, é€Ÿåº¦ -20%

---

### 3.2 â­â­ æ›´å¥½çš„ Patch Embedding

**å½“å‰**: ç®€å•çš„ 4Ã—4 å·ç§¯  
**æ¨è**: é‡å çš„å·ç§¯ + æ›´æ·±çš„ stem

```python
class AdvancedPatchEmbed(nn.Module):
    """
    æ›´å¼ºçš„ Patch Embedding
    ä½¿ç”¨é‡å å·ç§¯ + æ›´æ·±çš„ stem
    """
    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):
        super().__init__()
        self.stem = nn.Sequential(
            # Stage 1: 3â†’64, 3Ã—3 conv, stride=1
            nn.Conv2d(in_chans, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.GELU(),
            
            # Stage 2: 64â†’128, 3Ã—3 conv, stride=2
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.GELU(),
            
            # Stage 3: 128â†’embed_dim, 3Ã—3 conv, stride=2
            nn.Conv2d(128, embed_dim, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(embed_dim),
        )
        
        self.num_patches = (img_size // 4) ** 2  # 32/4 = 8, 8*8=64
    
    def forward(self, x):
        x = self.stem(x)  # [B, C, H, W]
        B, C, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]
        return x
```

**é¢„æœŸæå‡**: +0.3-0.7%  
**æˆæœ¬**: å‚æ•°é‡ +5%, é€Ÿåº¦åŸºæœ¬ä¸å˜

---

### 3.3 â­â­ Multi-scale SSM

**åŸç†**: åœ¨ä¸åŒå°ºåº¦ä¸Šåº”ç”¨ SSMï¼ˆç±»ä¼¼ FPNï¼‰

```python
class MultiScaleSSM(nn.Module):
    """å¤šå°ºåº¦ SSM"""
    def __init__(self, d_model, scales=[1, 2, 4]):
        super().__init__()
        self.scales = scales
        self.ssms = nn.ModuleList([
            SelectiveSSM(d_model) for _ in scales
        ])
        self.merge = nn.Linear(d_model * len(scales), d_model)
    
    def forward(self, x):
        # x: [B, L, D]
        outputs = []
        
        for i, scale in enumerate(self.scales):
            if scale == 1:
                out = self.ssms[i](x)
            else:
                # ä¸‹é‡‡æ · â†’ SSM â†’ ä¸Šé‡‡æ ·
                x_down = F.avg_pool1d(x.transpose(1, 2), kernel_size=scale).transpose(1, 2)
                out_down = self.ssms[i](x_down)
                out = F.interpolate(out_down.transpose(1, 2), size=x.size(1)).transpose(1, 2)
            
            outputs.append(out)
        
        # èåˆå¤šå°ºåº¦ç‰¹å¾
        merged = self.merge(torch.cat(outputs, dim=-1))
        return merged
```

**é¢„æœŸæå‡**: +0.5-1%  
**æˆæœ¬**: å‚æ•°é‡ +200%, é€Ÿåº¦ -30%

---

## ğŸ’» ä¼˜å…ˆçº§ 4: å·¥ç¨‹ä¼˜åŒ–ï¼ˆæé€Ÿä¸æç²¾åº¦ï¼‰

### 4.1 â­â­ Gradient Accumulationï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ï¼‰

```python
ACCUMULATION_STEPS = 4  # ç´¯ç§¯4ä¸ªbatch

for i, (images, targets) in enumerate(train_loader):
    outputs = model(images)
    loss = criterion(outputs, targets)
    loss = loss / ACCUMULATION_STEPS  # ç¼©æ”¾æŸå¤±
    
    loss.backward()
    
    if (i + 1) % ACCUMULATION_STEPS == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æ•ˆæœ**: ç­‰æ•ˆ batch_size Ã— 4ï¼Œæ˜¾å­˜éœ€æ±‚ä¸å˜

---

### 4.2 â­â­ ç¼–è¯‘ä¼˜åŒ– (torch.compile)

```python
# PyTorch 2.0+ æ”¯æŒ
model = torch.compile(model, mode='max-autotune')
```

**æ•ˆæœ**: é€Ÿåº¦æå‡ 10-30%ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰

---

### 4.3 â­ DataLoader ä¼˜åŒ–

```python
train_loader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    shuffle=True,
    num_workers=8,  # å¢åŠ  workers
    pin_memory=True,
    persistent_workers=True,  # æ–°å¢ï¼šä¿æŒ workers å¸¸é©»
    prefetch_factor=2,  # æ–°å¢ï¼šé¢„å–2ä¸ªbatch
)
```

**æ•ˆæœ**: æ•°æ®åŠ è½½åŠ é€Ÿ 20-40%

---

## ğŸ“‹ ä¼˜åŒ–å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: å¿«é€Ÿè§æ•ˆï¼ˆ1-2å¤©ï¼‰

```
1. âœ… æ·»åŠ  Drop Path            â†’ +0.5-1%
2. âœ… æ·»åŠ  Random Erasing       â†’ +0.3-0.5%
3. âœ… æ·»åŠ  Gradient Clipping    â†’ ç¨³å®šæ€§æå‡
4. âœ… æ·»åŠ  EMA                  â†’ +0.3-0.7%

é¢„æœŸæ€»æå‡: +1.1-2.2%
```

### é˜¶æ®µ 2: ä¸­æœŸä¼˜åŒ–ï¼ˆ3-5å¤©ï¼‰

```
5. âœ… æ··åˆç²¾åº¦è®­ç»ƒ (AMP)        â†’ é€Ÿåº¦ +100-200%
6. âœ… AutoAugment æ›¿æ¢ RandAugment â†’ +0.5-1%
7. âœ… Layer-wise LR Decay       â†’ +0.3-0.5%

é¢„æœŸæ€»æå‡: +0.8-1.5% + 2Ã— é€Ÿåº¦
```

### é˜¶æ®µ 3: é«˜çº§ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

```
8. âœ… Bidirectional SSM         â†’ +1-2%
9. âœ… æ”¹è¿› Patch Embedding      â†’ +0.3-0.7%
10. âœ… TTA (æµ‹è¯•æ—¶å¢å¼º)          â†’ +0.5-1%
11. âœ… Knowledge Distillation   â†’ +0.5-1%

é¢„æœŸæ€»æå‡: +2.3-4.7%
```

---

## ğŸ¯ æœ€ç»ˆé¢„æœŸæ€§èƒ½

| æ•°æ®é›† | å½“å‰ | é˜¶æ®µ1 | é˜¶æ®µ2 | é˜¶æ®µ3 | ç›®æ ‡ |
|--------|-----|-------|-------|-------|------|
| **CIFAR-10** | 94-95.5% | 95.5-97% | 96-97.5% | **97-98%** | 98%+ |
| **CIFAR-100** | 76-81% | 77-83% | 78-84% | **82-86%** | 85%+ |

---

## ğŸ“¦ ä¸€é”®ä¼˜åŒ–è„šæœ¬ï¼ˆå³å°†æä¾›ï¼‰

```bash
# é˜¶æ®µ1ä¼˜åŒ–ï¼ˆæ¨èå…ˆåšï¼‰
./run_mamba_baseline_v2.sh --stage 1

# é˜¶æ®µ2ä¼˜åŒ–
./run_mamba_baseline_v2.sh --stage 2

# é˜¶æ®µ3ä¼˜åŒ–ï¼ˆéœ€è¦æ›´å¤šæ—¶é—´ï¼‰
./run_mamba_baseline_v2.sh --stage 3
```

---

## ğŸ“š å‚è€ƒè®ºæ–‡

1. **DropPath**: Deep Networks with Stochastic Depth
2. **EMA**: Mean teachers are better role models
3. **Layer-wise LR**: ELECTRA: Pre-training Text Encoders
4. **Bidirectional SSM**: Vim: Vision Mamba
5. **TTA**: Test-Time Augmentation with Transformers

---

**æœ€åæ›´æ–°**: 2026-01-19  
**ä¸‹ä¸€æ­¥**: å®ç°é˜¶æ®µ1ä¼˜åŒ–
