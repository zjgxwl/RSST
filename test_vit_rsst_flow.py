"""
å¿«é€Ÿæµ‹è¯•ViT + RSSTåœ¨CIFAR-10ä¸Šçš„å®Œæ•´æµç¨‹
ç”¨äºéªŒè¯æµç¨‹æ˜¯å¦é€šç•…ï¼Œä¸è¿½æ±‚æœ€ç»ˆç²¾åº¦
"""
import os
import sys
import torch


def test_flow():
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    
    print("=" * 80)
    print("ViT + RSST on CIFAR-10 æµç¨‹æµ‹è¯•")
    print("=" * 80)
    print()
    
    # æµ‹è¯•1: å¯¼å…¥æ£€æŸ¥
    print("ã€æµ‹è¯•1/6ã€‘æ£€æŸ¥ä¾èµ–åº“")
    print("-" * 80)
    try:
        import torch
        import torchvision
        import timm
        print(f"âœ“ PyTorch: {torch.__version__}")
        print(f"âœ“ Torchvision: {torchvision.__version__}")
        print(f"âœ“ timm: {timm.__version__}")
        print(f"âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    except ImportError as e:
        print(f"âœ— ç¼ºå°‘ä¾èµ–: {e}")
        return False
    print()
    
    # æµ‹è¯•2: æ¨¡å‹åŠ è½½
    print("ã€æµ‹è¯•2/6ã€‘åŠ è½½é¢„è®­ç»ƒViTæ¨¡å‹")
    print("-" * 80)
    try:
        from models.vit import vit_tiny
        model = vit_tiny(num_classes=10, img_size=32, pretrained=True)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ“ æ€»å‚æ•°é‡: {total_params:,}")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    print()
    
    # æµ‹è¯•3: æ•°æ®åŠ è½½
    print("ã€æµ‹è¯•3/6ã€‘åŠ è½½CIFAR-10æ•°æ®é›†")
    print("-" * 80)
    try:
        from dataset import cifar10_dataloaders
        train_loader, val_loader, test_loader = cifar10_dataloaders(
            batch_size=32, 
            data_dir='data'
        )
        print(f"âœ“ è®­ç»ƒé›†batchæ•°: {len(train_loader)}")
        print(f"âœ“ éªŒè¯é›†batchæ•°: {len(val_loader)}")
        print(f"âœ“ æµ‹è¯•é›†batchæ•°: {len(test_loader)}")
        
        # æµ‹è¯•ä¸€ä¸ªbatch
        images, labels = next(iter(train_loader))
        print(f"âœ“ Batchå½¢çŠ¶: {images.shape}")
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False
    print()
    
    # æµ‹è¯•4: å‰å‘ä¼ æ’­
    print("ã€æµ‹è¯•4/6ã€‘æµ‹è¯•å‰å‘ä¼ æ’­")
    print("-" * 80)
    try:
        model.eval()
        with torch.no_grad():
            output = model(images)
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ“ é¢„æœŸå½¢çŠ¶: (32, 10)")
        assert output.shape == (32, 10), "è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®!"
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    print()
    
    # æµ‹è¯•5: å‰ªæåŠŸèƒ½
    print("ã€æµ‹è¯•5/6ã€‘æµ‹è¯•å‰ªæåŠŸèƒ½")
    print("-" * 80)
    try:
        import vit_pruning_utils
        
        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«ä¸ºViT
        is_vit = vit_pruning_utils.is_vit_model(model)
        print(f"âœ“ æ¨¡å‹è¯†åˆ«: {'ViT' if is_vit else 'CNN'}")
        assert is_vit, "æ¨¡å‹æœªè¢«è¯†åˆ«ä¸ºViT!"
        
        # æ£€æŸ¥åˆå§‹ç¨€ç–åº¦
        print("\nåˆå§‹ç¨€ç–åº¦:")
        remain_before = vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
        
        # æ‰§è¡Œå‰ªæ
        print(f"\næ‰§è¡Œ20%å‰ªæ...")
        vit_pruning_utils.pruning_model_vit(model, px=0.2, prune_patch_embed=False)
        
        # æ£€æŸ¥å‰ªæåç¨€ç–åº¦
        print("\nå‰ªæåç¨€ç–åº¦:")
        remain_after = vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
        
        print(f"\nâœ“ å‰ªæå‰å‰©ä½™: {remain_before:.2f}%")
        print(f"âœ“ å‰ªæåå‰©ä½™: {remain_after:.2f}%")
        print(f"âœ“ å®é™…å‰ªæ‰: {remain_before - remain_after:.2f}%")
        
    except Exception as e:
        print(f"âœ— å‰ªæåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    print()
    
    # æµ‹è¯•6: å‰ªæåæ¨ç†
    print("ã€æµ‹è¯•6/6ã€‘æµ‹è¯•å‰ªæåæ¨ç†")
    print("-" * 80)
    try:
        model.eval()
        with torch.no_grad():
            output = model(images)
        print(f"âœ“ å‰ªæåè¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ“ è¾“å‡ºæ— NaN: {not torch.isnan(output).any()}")
        print(f"âœ“ è¾“å‡ºæ— Inf: {not torch.isinf(output).any()}")
    except Exception as e:
        print(f"âœ— å‰ªæåæ¨ç†å¤±è´¥: {e}")
        return False
    print()
    
    return True


def run_quick_training():
    """è¿è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ1è½®å‰ªæï¼Œ2ä¸ªepochï¼‰"""
    print("=" * 80)
    print("è¿è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•")
    print("=" * 80)
    print()
    
    print("é…ç½®:")
    print("  - æ•°æ®é›†: CIFAR-10")
    print("  - æ¨¡å‹: ViT-Tiny (é¢„è®­ç»ƒ)")
    print("  - å‰ªææ¬¡æ•°: 2è½®")
    print("  - æ¯è½®epoch: 2")
    print("  - Batch size: 64")
    print()
    
    cmd = """
python main_imp_fillback.py \\
    --dataset cifar10 \\
    --arch vit_tiny \\
    --pretrained \\
    --struct rsst \\
    --criteria l1 \\
    --epochs 2 \\
    --batch_size 64 \\
    --lr 0.001 \\
    --warmup 1 \\
    --pruning_times 2 \\
    --rate 0.2 \\
    --RST_schedule exp_custom_exponents \\
    --reg_granularity_prune 0.5 \\
    --exponents 3 \\
    --save_dir test_output/vit_rsst_flow_test \\
    --seed 42
    """.strip()
    
    print("è¿è¡Œå‘½ä»¤:")
    print(cmd)
    print()
    print("=" * 80)
    print("å¼€å§‹è®­ç»ƒ... (é¢„è®¡5-10åˆ†é’Ÿ)")
    print("=" * 80)
    
    import subprocess
    result = subprocess.run(cmd, shell=True)
    
    return result.returncode == 0


if __name__ == '__main__':
    print("\n" + "ğŸ§ª" * 40)
    print("ViT + RSST æµç¨‹å¿«é€Ÿæµ‹è¯•")
    print("ğŸ§ª" * 40 + "\n")
    
    # é˜¶æ®µ1: ç»„ä»¶æµ‹è¯•
    print("é˜¶æ®µ1: ç»„ä»¶åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    success = test_flow()
    
    if not success:
        print("\n" + "âŒ" * 40)
        print("æµç¨‹æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("âŒ" * 40 + "\n")
        sys.exit(1)
    
    print("=" * 80)
    print("âœ“âœ“âœ“ æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡ï¼ âœ“âœ“âœ“")
    print("=" * 80)
    print()
    
    # è¯¢é—®æ˜¯å¦è¿è¡Œå®Œæ•´è®­ç»ƒæµ‹è¯•
    print("=" * 80)
    print("é˜¶æ®µ2: å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•ï¼ˆå¯é€‰ï¼‰")
    print("=" * 80)
    print()
    print("æ˜¯å¦è¿è¡Œå®Œæ•´è®­ç»ƒæµ‹è¯•ï¼Ÿ")
    print("  - è¿™å°†è¿è¡Œ2è½®å‰ªæï¼Œæ¯è½®2ä¸ªepoch")
    print("  - é¢„è®¡è€—æ—¶: 5-10åˆ†é’Ÿ")
    print("  - ç›®çš„: éªŒè¯è®­ç»ƒå¾ªç¯æ— æŠ¥é”™")
    print()
    
    response = input("è¿è¡Œå®Œæ•´æµ‹è¯•? [y/N]: ").strip().lower()
    
    if response == 'y':
        print()
        success = run_quick_training()
        
        if success:
            print("\n" + "âœ…" * 40)
            print("å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡ï¼")
            print("âœ…" * 40 + "\n")
            print("ç°åœ¨å¯ä»¥è¿è¡Œæ­£å¼å®éªŒäº†:")
            print()
            print("python main_imp_fillback.py \\")
            print("    --dataset cifar10 \\")
            print("    --arch vit_tiny \\")
            print("    --pretrained \\")
            print("    --struct rsst \\")
            print("    --epochs 80 \\")
            print("    --pruning_times 15")
            print()
        else:
            print("\n" + "âŒ" * 40)
            print("è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥ï¼")
            print("âŒ" * 40 + "\n")
            sys.exit(1)
    else:
        print()
        print("è·³è¿‡å®Œæ•´è®­ç»ƒæµ‹è¯•")
        print()
        print("æ‰‹åŠ¨è¿è¡Œå®Œæ•´æµ‹è¯•:")
        print()
        print("python main_imp_fillback.py \\")
        print("    --dataset cifar10 \\")
        print("    --arch vit_tiny \\")
        print("    --pretrained \\")
        print("    --struct rsst \\")
        print("    --epochs 2 \\")
        print("    --pruning_times 2 \\")
        print("    --batch_size 64")
        print()

