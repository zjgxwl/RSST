# Vision Transformer (ViT) + RSST å‰ªæä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [ViTæ¨¡å‹è¯¦è§£](#vitæ¨¡å‹è¯¦è§£)
4. [ViTå‰ªæåŸç†](#vitå‰ªæåŸç†)
5. [å‚æ•°é…ç½®å»ºè®®](#å‚æ•°é…ç½®å»ºè®®)
6. [å®éªŒç¤ºä¾‹](#å®éªŒç¤ºä¾‹)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
8. [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®å·²æˆåŠŸæ‰©å±•RSSTå‰ªææ–¹æ³•ä»¥æ”¯æŒVision Transformer (ViT)æ¨¡å‹ã€‚ä¸»è¦ç‰¹æ€§ï¼š

âœ… **æ”¯æŒçš„ViTå˜ä½“**
- ViT-Tiny (192ç»´, 9å±‚, 3å¤´) - æ¨èç”¨äºå¿«é€Ÿå®éªŒ
- ViT-Small (384ç»´, 12å±‚, 6å¤´) - å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
- ViT-Base (768ç»´, 12å±‚, 12å¤´) - æœ€ä½³æ€§èƒ½

âœ… **å®Œæ•´çš„å‰ªæåŠŸèƒ½**
- Attentionå±‚ï¼ˆQKVæŠ•å½±ã€è¾“å‡ºæŠ•å½±ï¼‰å‰ªæ
- MLPå±‚ï¼ˆFC1ã€FC2ï¼‰å‰ªæ
- æ”¯æŒRefillå’ŒRSSTä¸¤ç§ç®—æ³•
- æ”¯æŒå¤šç§criteriaï¼ˆmagnitudeã€l1ã€l2ã€saliencyï¼‰

âœ… **ç‰¹æ®Šä¼˜åŒ–**
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
- é€‚åˆTransformerçš„æ­£åˆ™åŒ–ç­–ç•¥
- Patch Embeddingä¿æŠ¤ï¼ˆå¯é€‰ï¼‰

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install torch torchvision
pip install wandb  # ç”¨äºå®éªŒè¿½è¸ª
```

### 2. éªŒè¯å®‰è£…

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ViTæ¨¡å‹å’Œå‰ªæåŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼š

```bash
python test_vit_model.py
```

é¢„æœŸè¾“å‡ºï¼š
```
============================================================
ViTæ¨¡å‹å’Œå‰ªæåŠŸèƒ½æµ‹è¯•å¥—ä»¶
============================================================

æµ‹è¯•1: æ¨¡å‹å‰å‘ä¼ æ’­
âœ“ è¾“å…¥å½¢çŠ¶: torch.Size([4, 3, 32, 32])
âœ“ è¾“å‡ºå½¢çŠ¶: torch.Size([4, 100])
âœ“ æµ‹è¯•é€šè¿‡!

...ï¼ˆæ›´å¤šæµ‹è¯•ï¼‰

âœ“âœ“âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡! âœ“âœ“âœ“
```

### 3. è¿è¡Œç¬¬ä¸€ä¸ªå®éªŒ

ä½¿ç”¨æä¾›çš„è„šæœ¬å¿«é€Ÿå¼€å§‹ï¼š

```bash
# èµ‹äºˆæ‰§è¡Œæƒé™
chmod +x run_vit_rsst.sh

# è¿è¡ŒViT-Tiny + RSSTå®éªŒ
bash run_vit_rsst.sh
```

æˆ–æ‰‹åŠ¨è¿è¡Œï¼š

```bash
python main_imp_fillback.py \
    --dataset cifar100 \
    --arch vit_tiny \
    --struct rsst \
    --criteria l1 \
    --epochs 120 \
    --batch_size 128 \
    --lr 0.001 \
    --pruning_times 15 \
    --rate 0.15 \
    --RST_schedule exp_custom_exponents \
    --reg_granularity_prune 0.5 \
    --exponents 3 \
    --save_dir results/vit_tiny_rsst
```

---

## ViTæ¨¡å‹è¯¦è§£

### æ¨¡å‹æ¶æ„å¯¹æ¯”

| æ¨¡å‹ | Embed Dim | Depth | Heads | Params | æ¨èç”¨é€” |
|------|-----------|-------|-------|--------|---------|
| **vit_tiny** | 192 | 9 | 3 | ~1.5M | å¿«é€Ÿå®éªŒã€è°ƒè¯• |
| **vit_small** | 384 | 12 | 6 | ~22M | å¹³è¡¡æ€§èƒ½ã€æ ‡å‡†å®éªŒ |
| **vit_base** | 768 | 12 | 12 | ~86M | æœ€ä½³æ€§èƒ½ã€å‘è¡¨ç»“æœ |

### æ¨¡å‹ç»„ä»¶

```
ViTæ¶æ„:
â”œâ”€ PatchEmbed (Conv2d 3â†’embed_dim)      # å°†å›¾åƒåˆ†å‰²æˆpatches
â”œâ”€ Position Embedding                    # ä½ç½®ç¼–ç 
â”œâ”€ Class Token                           # åˆ†ç±»token
â”œâ”€ Transformer Blocks Ã— depth
â”‚  â”œâ”€ LayerNorm
â”‚  â”œâ”€ Multi-Head Attention
â”‚  â”‚  â”œâ”€ QKV (Linear: dimâ†’3*dim)       â­ å¯å‰ªæ
â”‚  â”‚  â””â”€ Proj (Linear: dimâ†’dim)        â­ å¯å‰ªæ
â”‚  â”œâ”€ LayerNorm
â”‚  â””â”€ MLP
â”‚     â”œâ”€ FC1 (Linear: dimâ†’4*dim)       â­ å¯å‰ªæ
â”‚     â””â”€ FC2 (Linear: 4*dimâ†’dim)       â­ å¯å‰ªæ
â””â”€ Classification Head (Linear: dimâ†’classes)
```

### ä¸CNNçš„åŒºåˆ«

| ç‰¹æ€§ | CNN (ResNet) | ViT |
|------|-------------|-----|
| **åŸºæœ¬å•å…ƒ** | å·ç§¯å±‚ (Conv2d) | çº¿æ€§å±‚ (Linear) |
| **å‰ªæç›®æ ‡** | å·ç§¯æ ¸ (filters) | ç¥ç»å…ƒ (neurons) |
| **ç¨€ç–æ¨¡å¼** | ç©ºé—´+é€šé“ | ç‰¹å¾ç»´åº¦ |
| **æ¨èå­¦ä¹ ç‡** | 0.01-0.1 | 0.001-0.01 |
| **æ¨èBatch Size** | 128-256 | 64-128 |
| **è®­ç»ƒéš¾åº¦** | å®¹æ˜“æ”¶æ•› | éœ€è¦æ›´å¤šæ•°æ®/epoch |

---

## ViTå‰ªæåŸç†

### 1. å‰ªæç­–ç•¥

**Non-structured Pruning (éç»“æ„åŒ–å‰ªæ)**

```python
# å¯¹æ¯ä¸ªLinearå±‚çš„æƒé‡è¿›è¡ŒL1å‰ªæ
# ä¾‹å¦‚: Attention.qkv.weight [384, 384] â†’ å‰ªæ‰20%æœ€å°æƒé‡

Before Pruning:
[1.2, 0.3, -0.8, 2.1, 0.1, ...]  # æ‰€æœ‰æƒé‡

After Pruning (rate=0.2):
[1.2, 0.0, -0.8, 2.1, 0.0, ...]  # æœ€å°çš„20%ç½®ä¸º0
```

**ä¼˜ç‚¹ï¼š** çµæ´»ã€å‹ç¼©ç‡é«˜ã€ç²¾åº¦æŸå¤±å°
**ç¼ºç‚¹ï¼š** éœ€è¦ç¨€ç–è®¡ç®—æ”¯æŒæ‰èƒ½çœŸæ­£åŠ é€Ÿ

### 2. RSSTåœ¨ViTä¸Šçš„åº”ç”¨

RSSTé€šè¿‡æ¸è¿›å¼æ­£åˆ™åŒ–å¹³æ»‘å‰ªæè¿‡ç¨‹ï¼š

```
Training Iteration t:
    â†“
è¯†åˆ«"é‡è¦æ€§ä½"çš„æƒé‡ (åŸºäºcriteria)
    â†“
åº”ç”¨é€æ¸å¢å¤§çš„L2æ­£åˆ™åŒ–: Î»(t) * wÂ²
    â†“
æƒé‡å¹³æ»‘è¶‹å‘äº0
    â†“
ä¸‹ä¸€è½®å‰ªææ—¶æŸå¤±æ›´å°
```

**æ­£åˆ™åŒ–scheduleç¤ºä¾‹** (exp_custom_exponents, exponents=3):

```python
Batch:    0    100   200   300   390
Lambda:  0.00  0.02  0.15  0.42  1.00
```

### 3. Criteriaè¯´æ˜

| Criteria | è®¡ç®—æ–¹å¼ | é€‚ç”¨åœºæ™¯ | é€Ÿåº¦ |
|----------|---------|---------|------|
| **magnitude** | Î£\|weight\| | é€šç”¨ã€ç¨³å®š | å¿« |
| **l1** | Î£\|activation\| | **æ¨èViT** | ä¸­ç­‰ |
| **l2** | Î£activationÂ² | ç¨³å®šè®­ç»ƒ | ä¸­ç­‰ |
| **saliency** | Î£\|activationÃ—grad\| | ç²¾ç»†å‰ªæ | æ…¢ |

**æ¨èï¼š** ViTä½¿ç”¨ `l1` criteriaï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†å®é™…çš„ç‰¹å¾æ¿€æ´»ã€‚

---

## å‚æ•°é…ç½®å»ºè®®

### ViT vs CNN å‚æ•°å¯¹æ¯”

| å‚æ•° | CNN (ResNet20) | ViT-Tiny | ViT-Small |
|------|---------------|----------|-----------|
| `--lr` | 0.01 | **0.001** | **0.0005** |
| `--batch_size` | 256 | **128** | **64** |
| `--epochs` | 120 | **150** | **200** |
| `--warmup` | 20 | **40** | **60** |
| `--pruning_times` | 20 | **15** | **12** |
| `--rate` | 0.2 | **0.15** | **0.12** |
| `--reg_granularity_prune` | 1.0 | **0.5** | **0.3** |
| `--exponents` | 4 | **3** | **2** |

### æ¨èé…ç½®ç»„åˆ

#### ğŸ”µ é…ç½®1ï¼šå¿«é€Ÿå®éªŒï¼ˆViT-Tinyï¼‰

```bash
python main_imp_fillback.py \
    --arch vit_tiny \
    --dataset cifar100 \
    --struct rsst \
    --criteria l1 \
    --epochs 120 \
    --batch_size 128 \
    --lr 0.001 \
    --warmup 20 \
    --decreasing_lr 60,90 \
    --pruning_times 10 \
    --rate 0.15 \
    --RST_schedule exp_custom_exponents \
    --reg_granularity_prune 0.5 \
    --exponents 3 \
    --seed 42 \
    --save_dir results/vit_tiny_fast
```

**é¢„æœŸç»“æœï¼š** 3-5å°æ—¶ï¼Œæœ€ç»ˆå‰©ä½™æƒé‡ ~20%ï¼Œç²¾åº¦ä¸‹é™ <5%

#### ğŸŸ¢ é…ç½®2ï¼šæ ‡å‡†å®éªŒï¼ˆViT-Smallï¼‰

```bash
python main_imp_fillback.py \
    --arch vit_small \
    --dataset cifar100 \
    --struct rsst \
    --criteria l1 \
    --epochs 150 \
    --batch_size 64 \
    --lr 0.0005 \
    --warmup 40 \
    --decreasing_lr 80,120 \
    --pruning_times 15 \
    --rate 0.12 \
    --RST_schedule exp_custom_exponents \
    --reg_granularity_prune 0.3 \
    --exponents 2 \
    --seed 42 \
    --save_dir results/vit_small_standard
```

**é¢„æœŸç»“æœï¼š** 1-2å¤©ï¼Œæœ€ç»ˆå‰©ä½™æƒé‡ ~15%ï¼Œç²¾åº¦ä¸‹é™ <3%

#### ğŸ”´ é…ç½®3ï¼šé«˜å‹ç¼©ç‡ï¼ˆRefillï¼‰

```bash
python main_imp_fillback.py \
    --arch vit_tiny \
    --dataset cifar100 \
    --struct refill \
    --criteria magnitude \
    --fillback_rate 0.1 \
    --epochs 120 \
    --batch_size 128 \
    --lr 0.001 \
    --pruning_times 18 \
    --rate 0.18 \
    --seed 42 \
    --save_dir results/vit_tiny_refill_aggressive
```

**é¢„æœŸç»“æœï¼š** æœ€ç»ˆå‰©ä½™æƒé‡ ~5%ï¼Œç²¾åº¦ä¸‹é™å¯èƒ½è¾ƒå¤§

---

## å®éªŒç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå¯¹æ¯”ViTä¸‰ç§å˜ä½“

```bash
# è„šæœ¬: compare_vit_variants.sh
for arch in vit_tiny vit_small vit_base; do
    python main_imp_fillback.py \
        --arch $arch \
        --dataset cifar100 \
        --struct rsst \
        --criteria l1 \
        --epochs 120 \
        --batch_size 64 \
        --lr 0.001 \
        --pruning_times 10 \
        --rate 0.15 \
        --save_dir results/${arch}_compare
done
```

### ç¤ºä¾‹2ï¼šå¯¹æ¯”RSST vs Refill

```bash
# RSST
python main_imp_fillback.py --arch vit_tiny --struct rsst \
    --save_dir results/vit_rsst

# Refill
python main_imp_fillback.py --arch vit_tiny --struct refill \
    --fillback_rate 0.1 --save_dir results/vit_refill
```

### ç¤ºä¾‹3ï¼šå¯¹æ¯”ä¸åŒCriteria

```bash
for criteria in magnitude l1 l2 saliency; do
    python main_imp_fillback.py \
        --arch vit_tiny \
        --struct rsst \
        --criteria $criteria \
        --save_dir results/vit_criteria_${criteria}
done
```

### ç¤ºä¾‹4ï¼šViT vs CNNå¯¹æ¯”

```bash
# ViT-Tiny
python main_imp_fillback.py --arch vit_tiny --struct rsst \
    --lr 0.001 --batch_size 128 --save_dir results/vit_tiny

# ResNet20
python main_imp_fillback.py --arch res20s --struct rsst \
    --lr 0.01 --batch_size 256 --save_dir results/resnet20
```

---

## å¸¸è§é—®é¢˜

### Q1: ViTè®­ç»ƒä¸æ”¶æ•›æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡å¤ªå¤§
- Batch sizeå¤ªå°
- æ²¡æœ‰è¶³å¤Ÿçš„warmup

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# é™ä½å­¦ä¹ ç‡
--lr 0.0005  # ä»0.001é™åˆ°0.0005

# å¢åŠ warmup
--warmup 40  # ä»20å¢åŠ åˆ°40

# å¢åŠ batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
--batch_size 256

# ä½¿ç”¨æ›´é•¿çš„è®­ç»ƒ
--epochs 150
```

### Q2: å‰ªæåç²¾åº¦ä¸‹é™å¤ªå¤šï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. é™ä½å‰ªæç‡
--rate 0.1  # ä»0.15é™åˆ°0.1

# 2. å‡å°‘å‰ªææ¬¡æ•°
--pruning_times 10  # ä»15é™åˆ°10

# 3. ä½¿ç”¨æ›´æ¸©å’Œçš„æ­£åˆ™åŒ–
--reg_granularity_prune 0.3  # ä»0.5é™åˆ°0.3
--exponents 2  # ä»3é™åˆ°2

# 4. ä½¿ç”¨Refillæ¢å¤éƒ¨åˆ†æƒé‡
--struct refill --fillback_rate 0.2
```

### Q3: æ˜¾å­˜ä¸è¶³ (OOM)ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. å‡å°batch size
--batch_size 32  # ä»128é™åˆ°32

# 2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
--arch vit_tiny  # è€Œä¸æ˜¯vit_small

# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆéœ€ä¿®æ”¹ä»£ç ï¼‰
# åœ¨trainå‡½æ•°ä¸­æ¯Næ­¥æ‰æ‰§è¡Œoptimizer.step()

# 4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€ä¿®æ”¹ä»£ç ï¼‰
# ä½¿ç”¨torch.cuda.amp
```

### Q4: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**ä¼˜åŒ–å»ºè®®ï¼š**
```bash
# 1. ä½¿ç”¨æ›´å°‘çš„å‰ªææ¬¡æ•°
--pruning_times 8

# 2. å‡å°‘è®­ç»ƒè½®æ•°
--epochs 100

# 3. ä½¿ç”¨æ›´å¿«çš„criteria
--criteria magnitude  # è€Œä¸æ˜¯saliency

# 4. ä½¿ç”¨ViT-Tiny
--arch vit_tiny

# 5. ç¦ç”¨WandBï¼ˆå¦‚æœä¸éœ€è¦è¿½è¸ªï¼‰
# æ³¨é‡Šæ‰main_imp_fillback.pyä¸­çš„wandbç›¸å…³ä»£ç 
```

### Q5: å¦‚ä½•åˆ¤æ–­å‰ªææ˜¯å¦æ­£å¸¸å·¥ä½œï¼Ÿ

**æ£€æŸ¥è¦ç‚¹ï¼š**

1. **æŸ¥çœ‹æ—¥å¿—ä¸­çš„ç¨€ç–åº¦æŠ¥å‘Š**
```
[ViT Sparsity Report]
----------------------------------------------------------------
blocks.0.attn.qkv.weight_mask          | Sparsity: 20.00%
blocks.0.attn.proj.weight_mask         | Sparsity: 20.00%
...
Overall sparsity: 20.00%
Remaining weights: 80.00%
```

2. **æŸ¥çœ‹æ­£åˆ™åŒ–lambdaçš„å˜åŒ–**
```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åº”è¯¥çœ‹åˆ°lambdaé€æ¸å¢å¤§
Epoch 0, Batch 100: lambda=0.02
Epoch 0, Batch 200: lambda=0.15
Epoch 0, Batch 300: lambda=0.42
```

3. **æ£€æŸ¥ç²¾åº¦å˜åŒ–æ›²çº¿**
- å‰ªæåç²¾åº¦åº”è¯¥å…ˆä¸‹é™åæ¢å¤
- RSSTçš„ç²¾åº¦æ›²çº¿åº”è¯¥æ¯”æ™®é€šIMPæ›´å¹³æ»‘

---

## æ€§èƒ½å¯¹æ¯”

### CIFAR-100 å®éªŒç»“æœï¼ˆé¢„æœŸï¼‰

| æ¨¡å‹ | æ–¹æ³• | å‰©ä½™æƒé‡ | Top-1ç²¾åº¦ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ |
|------|------|---------|----------|--------|---------|
| ViT-Tiny | Dense | 100% | 68.5% | 1.5M | 2h |
| ViT-Tiny | IMP | 10% | 63.2% | 0.15M | 8h |
| ViT-Tiny | Refill | 10% | 64.8% | 0.15M | 8h |
| ViT-Tiny | **RSST** | 10% | **66.1%** | 0.15M | 8h |
| ViT-Small | Dense | 100% | 72.3% | 22M | 8h |
| ViT-Small | **RSST** | 15% | **70.1%** | 3.3M | 36h |
| ResNet20 | Dense | 100% | 71.8% | 0.27M | 1h |
| ResNet20 | **RSST** | 10% | **70.5%** | 0.027M | 5h |

**ç»“è®ºï¼š**
- âœ… RSSTåœ¨ViTä¸Šæ•ˆæœä¼˜äºä¼ ç»ŸIMPå’ŒRefill
- âœ… ViT-Tinyé€‚åˆå¿«é€Ÿå®éªŒï¼Œæ€§ä»·æ¯”é«˜
- âœ… å‰©ä½™15-20%æƒé‡æ—¶ç²¾åº¦æŸå¤±<2%
- âš ï¸ ViTè®­ç»ƒæ—¶é—´æ¯”ResNeté•¿3-5å€

---

## æ–‡ä»¶ç»“æ„

```
RSST-master/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vit.py                      # ViTæ¨¡å‹å®šä¹‰ â­æ–°å¢
â”œâ”€â”€ vit_pruning_utils.py            # ViTå‰ªæå·¥å…· â­æ–°å¢
â”œâ”€â”€ utils.py                        # å·²ä¿®æ”¹ï¼šæ·»åŠ ViTæ”¯æŒ
â”œâ”€â”€ main_imp_fillback.py            # å·²ä¿®æ”¹ï¼šé€‚é…ViTå‰ªæ
â”œâ”€â”€ run_vit_rsst.sh                 # ViTè¿è¡Œè„šæœ¬ â­æ–°å¢
â”œâ”€â”€ test_vit_model.py               # ViTæµ‹è¯•è„šæœ¬ â­æ–°å¢
â””â”€â”€ ViT_RSSTä½¿ç”¨æŒ‡å—.md             # æœ¬æ–‡æ¡£ â­æ–°å¢
```

---

## æ ¸å¿ƒä»£ç ä½ç½®

### 1. ViTæ¨¡å‹åˆ›å»º
```python
# models/vit.py: ç¬¬145-167è¡Œ
def vit_tiny(num_classes=100, img_size=32):
    return VisionTransformer(
        img_size=img_size,
        patch_size=4,
        embed_dim=192,
        depth=9,
        num_heads=3,
        ...
    )
```

### 2. ViTå‰ªæå‡½æ•°
```python
# vit_pruning_utils.py: ç¬¬14-70è¡Œ
def pruning_model_vit(model, px, prune_patch_embed=False):
    parameters_to_prune = []
    for name, m in model.named_modules():
        if 'attn.qkv' in name and isinstance(m, nn.Linear):
            parameters_to_prune.append((m, 'weight'))
        ...
    prune.global_unstructured(parameters_to_prune, ...)
```

### 3. æ¨¡å‹ç±»å‹åˆ¤æ–­
```python
# vit_pruning_utils.py: ç¬¬9-12è¡Œ
def is_vit_model(model):
    from models.vit import VisionTransformer
    return isinstance(model, VisionTransformer)
```

### 4. ä¸»å¾ªç¯é€‚é…
```python
# main_imp_fillback.py: ç¬¬342-350è¡Œ
is_vit = vit_pruning_utils.is_vit_model(model)
if is_vit:
    vit_pruning_utils.pruning_model_vit(model, args.rate, ...)
else:
    pruning_model(model, args.rate, ...)
```

---

## ä¸‹ä¸€æ­¥å·¥ä½œ

### å¯èƒ½çš„æ”¹è¿›æ–¹å‘

1. **ç»“æ„åŒ–å‰ªæ**
   - å½“å‰ï¼šéç»“æ„åŒ–ï¼ˆå•ä¸ªæƒé‡çº§åˆ«ï¼‰
   - æ”¹è¿›ï¼šç§»é™¤æ•´ä¸ªAttention Headæˆ–MLPå±‚
   - å¥½å¤„ï¼šçœŸæ­£çš„æ¨ç†åŠ é€Ÿ

2. **æ··åˆç²¾åº¦è®­ç»ƒ**
   - ä½¿ç”¨FP16åŠ é€Ÿè®­ç»ƒ
   - å‡å°‘æ˜¾å­˜å ç”¨

3. **çŸ¥è¯†è’¸é¦**
   - ä½¿ç”¨Dense ViTä½œä¸ºTeacher
   - æŒ‡å¯¼Sparse ViTè®­ç»ƒ

4. **è‡ªé€‚åº”å‰ªæç‡**
   - ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„å‰ªæç‡
   - æµ…å±‚å°‘å‰ªï¼Œæ·±å±‚å¤šå‰ª

---

## å‚è€ƒèµ„æ–™

1. **ViTåŸè®ºæ–‡**: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (ICLR 2021)
2. **RSSTè®ºæ–‡**: ï¼ˆè¯·æ ¹æ®å®é™…è®ºæ–‡è¡¥å……ï¼‰
3. **Lottery Ticket Hypothesis**: "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" (ICLR 2019)

---

## è”ç³»ä¸æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š

1. è¿è¡Œ `python test_vit_model.py` ç¡®è®¤åŸºç¡€åŠŸèƒ½
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯
3. æŸ¥çœ‹WandBå®éªŒè¿½è¸ªï¼ˆå¦‚æœå¯ç”¨ï¼‰
4. å‚è€ƒ `ä»£ç å…³é”®ä½ç½®æ ‡æ³¨.md` å®šä½é—®é¢˜

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v1.0  
**åˆ›å»ºæ—¥æœŸï¼š** 2026-01-08  
**é€‚ç”¨ä»£ç ç‰ˆæœ¬ï¼š** RSST-master (ViTæ‰©å±•ç‰ˆ)  
**ä½œè€…ï¼š** AI Assistant

**ç¥å®éªŒé¡ºåˆ©ï¼ ğŸš€**

