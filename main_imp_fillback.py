'''
iterative pruning for supervised task 
with lottery tickets or pretrain tickets 
support datasets: cifar10, Fashionmnist, cifar100
'''

import os
import pdb
import time 
import pickle
import random
import shutil
import argparse
import numpy as np  
from copy import deepcopy
import matplotlib.pyplot as plt
import math
import torch
import torch.optim
import torch.nn as nn
import torch.utils.data
import torch.nn.functional as F
import torchvision.models as models
import torch.backends.cudnn as cudnn
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data.sampler import SubsetRandomSampler
# from advertorch.utils import NormalizeByChannelMeanStd
from normalize_utils import NormalizeByChannelMeanStd  # è‡ªå®šä¹‰å®ç°ï¼ŒåŠŸèƒ½ç›¸åŒ
from utils import *
from pruning_utils import *
import vit_pruning_utils
import vit_structured_pruning  # æ–°å¢ï¼šç»“æ„åŒ–å‰ªææ¨¡å—
import vit_pruning_utils_head_mlp  # æ–°å¢ï¼šHead+MLPç»„åˆå‰ªææ¨¡å—

from reg_pruner_files import reg_pruner
# æ³¨é‡Šæ‰wandbä»¥æå‡è®­ç»ƒé€Ÿåº¦
# import wandb

# è®¾ç½® CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡
# os.environ["CUDA_VISIBLE_DEVICES"] = '0'  # å·²æ³¨é‡Šï¼šç”±å¯åŠ¨è„šæœ¬æ§åˆ¶GPUåˆ†é…
# æ³¨é‡Šæ‰wandbç¯å¢ƒå˜é‡è®¾ç½®ä»¥æå‡è®­ç»ƒé€Ÿåº¦
# è®¾ç½® WANDB_API_KEY ç¯å¢ƒå˜é‡
# os.environ["WANDB_API_KEY"] = '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'


parser = argparse.ArgumentParser(description='PyTorch Iterative Pruning')

##################################### general setting #################################################
parser.add_argument('--data', type=str, default='data', help='location of the data corpus')
parser.add_argument('--dataset', type=str, default='cifar100', help='dataset')
parser.add_argument('--arch', type=str, default='res20s', help='model architecture')
parser.add_argument('--vit_pretrained', action='store_true', help='use pretrained model (for ViT)')
parser.add_argument('--file_name', type=str, default=None, help='dataset index')
parser.add_argument('--seed', default=None, type=int, help='random seed')
parser.add_argument('--save_dir', help='The directory used to save the trained models', default='cifar100_rsst_output_resnet20_l1_exp_custom_exponents4', type=str)
parser.add_argument('--exp_name', type=str, default=None, help='custom experiment name for wandb (auto-generate if None)')
parser.add_argument('--gpu', type=int, default=0, help='gpu device id')
parser.add_argument('--resume', action="store_true", help="resume from checkpoint")
parser.add_argument('--checkpoint', type=str, default=None, help='checkpoint file')
parser.add_argument('--init', type=str, default='init_model/cifar100_output_resnet20_l1_x_init.pth.tar', help='init file')

##################################### training setting #################################################
parser.add_argument('--batch_size', type=int, default=128, help='batch size')
parser.add_argument('--workers', type=int, default=4, help='number of data loading workers')
parser.add_argument('--lr', default=0.01, type=float, help='initial learning rate')
parser.add_argument('--momentum', default=0.9, type=float, help='momentum')
parser.add_argument('--weight_decay', default=1e-4, type=float, help='weight decay')
parser.add_argument('--epochs', default=120, type=int, help='number of total epochs to run')
parser.add_argument('--warmup', default=20, type=int, help='warm up epochs')
parser.add_argument('--print_freq', default=200, type=int, help='print frequency')
parser.add_argument('--decreasing_lr', default='91,136', help='decreasing strategy')

##################################### Pruning setting #################################################
parser.add_argument('--pruning_times', default=20, type=int, help='overall times of pruning')
parser.add_argument('--rate', default=0.2, type=float, help='pruning rate')
parser.add_argument('--prune_type', default='lt', type=str, help='IMP type (lt,pt or pt_trans)')
parser.add_argument('--pretrained', default=None, type=str, help='pretrained weight for pt')
parser.add_argument('--conv1', action="store_true", help="whether pruning&rewind conv1")
parser.add_argument('--fc', action="store_true", help="whether rewind fc")
parser.add_argument('--rewind_epoch', default=24, type=int, help='rewind checkpoint')



parser.add_argument('--struct', default='rsst', type=str, choices=['refill','rsst'], help='overall times of pruning')
parser.add_argument('--fillback_rate', default=0.0, type=float)
parser.add_argument('--block_loss_grad', default=False, help="block the grad from loss, only apply weight decay")
parser.add_argument('--RST_schedule', type=str, default='exp_custom_exponents', choices=['x', 'x^2', 'x^3', 'exp', 'exp_custom','exp_custom_exponents' ])
parser.add_argument('--reg_granularity_prune', type=float, default=1, help='æ­£åˆ™åŒ–é˜ˆå€¼')
parser.add_argument('--criteria', default="l1", type=str, choices=['remain', 'magnitude', 'l1', 'l2', 'saliency'])
parser.add_argument('--exponents', default=4, type=int, help='æ­¤å‚æ•°ç”¨æ¥æ§åˆ¶æŒ‡æ•°å‡½æ•°çš„æ›²ç‡' )
parser.add_argument('--vit_structured', action='store_true', help='use structured pruning for ViT (head-level, not element-wise)')
parser.add_argument('--vit_prune_target', default='head', type=str, choices=['head', 'mlp', 'both'], 
                    help='ViT structured pruning target: head (attention heads), mlp (MLP neurons), both (head+mlp)')
parser.add_argument('--mlp_prune_ratio', default=None, type=float, 
                    help='MLP neuron pruning ratio (default: use same as --rate)')
parser.add_argument('--sorting_mode', default='layer-wise', type=str, choices=['layer-wise', 'global'],
                    help='Sorting mode for ViT structured pruning: layer-wise (each layer independently) or global (all layers mixed)')
best_sa = 0

def main():
    global args, best_sa
    args = parser.parse_args()
    args.use_sparse_conv = False
    print(args)
    
    # æ³¨é‡Šæ‰WandBåˆå§‹åŒ–ä»¥æå‡è®­ç»ƒé€Ÿåº¦
    # åˆå§‹åŒ–WandB - çµæ´»çš„å®éªŒåç§°
    # if args.exp_name:
    #     # ç”¨æˆ·è‡ªå®šä¹‰åç§°
    #     wdb_name = args.exp_name
    # else:
    #     # è‡ªåŠ¨ç”Ÿæˆåç§°
    #     import datetime
    #     timestamp = datetime.datetime.now().strftime("%m%d_%H%M")
    #     
    #     # åŸºç¡€ä¿¡æ¯
    #     name_parts = [args.struct, args.arch, args.dataset]
    #     
    #     # æ·»åŠ å…³é”®å‚æ•°
    #     if args.struct == 'rsst':
    #         name_parts.append(f"sched_{args.RST_schedule}")
    #         name_parts.append(f"reg_{args.reg_granularity_prune}")
    #         if args.RST_schedule == 'exp_custom_exponents':
    #             name_parts.append(f"exp{args.exponents}")
    #     elif args.struct == 'refill':
    #         name_parts.append(f"fill_{args.fillback_rate}")
    #     
    #     name_parts.append(f"crit_{args.criteria}")
    #     name_parts.append(f"rate_{args.rate}")
    #     
    #     # æ·»åŠ é¢„è®­ç»ƒæ ‡è¯†
    #     if hasattr(args, 'vit_pretrained') and args.vit_pretrained:
    #         name_parts.append("pretrained")
    #     
    #     # æ·»åŠ ç»“æ„åŒ–å‰ªææ ‡è¯†
    #     if hasattr(args, 'vit_structured') and args.vit_structured:
    #         if hasattr(args, 'vit_prune_target'):
    #             name_parts.append(f"struct_{args.vit_prune_target}")
    #         else:
    #             name_parts.append("struct_head")
    #     
    #     # æ·»åŠ æ—¶é—´æˆ³
    #     name_parts.append(timestamp)
    #     
    #     wdb_name = '_'.join(name_parts)
    # 
    # print(f'WandBå®éªŒåç§°: {wdb_name}')
    # wandb.init(project='RSST', entity='ycx', name=wdb_name, config=vars(parser.parse_args()))
    print('*'*50)
    print('conv1 included for prune and rewind: {}'.format(args.conv1))
    print('fc included for rewind: {}'.format(args.fc))
    print('*'*50)

    torch.cuda.set_device(int(args.gpu))
    os.makedirs(args.save_dir, exist_ok=True)
    if args.seed:
        setup_seed(args.seed)

    # prepare dataset 
    model, train_loader, val_loader, test_loader = setup_model_dataset(args)
    # æ³¨é‡Šæ‰wandb.watchä»¥æå‡è®­ç»ƒé€Ÿåº¦
    # wandb.watch(model, log='all', log_freq=1, log_graph=True)
    model.cuda()


    criterion = nn.CrossEntropyLoss()
    decreasing_lr = list(map(int, args.decreasing_lr.split(',')))

    # å£°æ˜ä¸€ä¸ªç±»ç”¨äºå‚æ•°ä¼ é€’
    class passer:
        pass
    #åˆå§‹åŒ–ç±»ä¸­å±æ€§
    passer.test = validate  # æµ‹è¯•å‡½æ•°
    passer.model = None
    passer.test_loader = val_loader  # éªŒè¯æ•°æ®åŠ è½½å™¨
    passer.criterion = criterion  # æŸå¤±å‡½æ•°
    passer.args = args  # å‚æ•°
    passer.current_mask = None
    passer.reg = {}  # ä¿å­˜æ¯å±‚çš„æ­£åˆ™åŒ–é¡¹
    passer.reg_indices = []
    passer.refill_mask = None # ä¸Šä¸€è½®è¿­ä»£ä¸­äº§ç”Ÿçš„é‡ç»„mask
    passer.reg_plot_dict = {} # æ”¶é›†æ¯ä¸€æ¬¡æ›´æ–°çš„regç”»å›¾

    optimizer = torch.optim.SGD(model.parameters(), args.lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decreasing_lr, gamma=0.1)
    
    print(model.normalize)  
    new_initialization = copy.deepcopy(model.state_dict())
    if not os.path.exists(args.init):
        torch.save(new_initialization, args.init)
    initialization = torch.load(args.init)
    if 'state_dict' in initialization:
        initialization = initialization['state_dict']
    
    # â­â­â­ éªŒè¯æ˜¯å¦ä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆé˜²æ­¢ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰â­â­â­
    if args.arch in ['vit_tiny', 'vit_small', 'vit_base'] and hasattr(args, 'vit_pretrained') and args.vit_pretrained:
        # æ£€æŸ¥ViTæ¨¡å‹æ˜¯å¦çœŸçš„ä½¿ç”¨äº†é¢„è®­ç»ƒæƒé‡
        test_key = 'blocks.0.attn.qkv.weight'
        if test_key in initialization:
            test_weight = initialization[test_key]
            weight_std = test_weight.std().item()
            
            print("="*80)
            print("ğŸ” é¢„è®­ç»ƒæ¨¡å‹éªŒè¯")
            print("="*80)
            print(f"åˆå§‹åŒ–æ–‡ä»¶: {args.init}")
            print(f"æµ‹è¯•å‚æ•°: {test_key}")
            print(f"æƒé‡std: {weight_std:.6f}")
            
            # Xavier/Kaimingéšæœºåˆå§‹åŒ–çš„stdé€šå¸¸åœ¨0.01-0.03èŒƒå›´
            # çœŸæ­£çš„é¢„è®­ç»ƒæƒé‡stdé€šå¸¸>0.05
            if weight_std < 0.05:
                print(f"âŒ é”™è¯¯ï¼šåˆå§‹åŒ–æ–‡ä»¶ç–‘ä¼¼éšæœºåˆå§‹åŒ–ï¼ˆstd={weight_std:.6f} < 0.05ï¼‰")
                print(f"âŒ æœŸæœ›ï¼šé¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼ˆstdåº”è¯¥ > 0.05ï¼‰")
                print("="*80)
                print("âš ï¸  å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
                print("   1. åˆ é™¤æ—§çš„åˆå§‹åŒ–æ–‡ä»¶")
                print("   2. é‡æ–°è¿è¡Œä»¥ç”ŸæˆçœŸæ­£çš„é¢„è®­ç»ƒåˆå§‹åŒ–æ–‡ä»¶")
                print("   3. æˆ–è€…ç§»é™¤ --vit_pretrained å‚æ•°ï¼ˆä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰")
                print("="*80)
                raise ValueError("åˆå§‹åŒ–æ–‡ä»¶ä¸æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼è¯·æ£€æŸ¥åˆå§‹åŒ–æ–‡ä»¶æˆ–ç§»é™¤ --vit_pretrained å‚æ•°")
            else:
                print(f"âœ“ éªŒè¯é€šè¿‡ï¼šç¡®è®¤æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼ˆstd={weight_std:.6f} > 0.05ï¼‰")
                print("="*80)
        else:
            print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°æµ‹è¯•å‚æ•° {test_key}ï¼Œè·³è¿‡éªŒè¯")
    
    initialization['normalize.mean'] = new_initialization['normalize.mean']
    initialization['normalize.std'] = new_initialization['normalize.std']

    print(initialization.keys())
    if not args.prune_type == 'lt':
        keys = list(initialization.keys())
        for key in keys:
            # è·³è¿‡åˆ†ç±»å¤´å’Œconv1ï¼ˆResNetç”¨fcï¼ŒViTç”¨headï¼‰
            if key.startswith('fc') or key.startswith('conv1') or key.startswith('head'):
                del initialization[key]

        # åˆ¤æ–­æ¨¡å‹ç±»å‹å¹¶æ¢å¤å¯¹åº”çš„åˆ†ç±»å¤´ï¼ˆäº’æ–¥é€‰æ‹©ï¼‰
        if 'head.weight' in new_initialization:
            # ViTæ¨¡å‹ - æ¢å¤headå±‚
            num_classes = new_initialization['head.weight'].shape[0]
            print(f"âœ“ æ£€æµ‹åˆ°ViTæ¨¡å‹ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–çš„headå±‚ï¼ˆç±»åˆ«æ•°ï¼š{num_classes}ï¼‰")
            initialization['head.weight'] = new_initialization['head.weight']
            initialization['head.bias'] = new_initialization['head.bias']
            
        elif 'fc.weight' in new_initialization:
            # ResNetæ¨¡å‹ - æ¢å¤fcå’Œconv1å±‚
            num_classes = new_initialization['fc.weight'].shape[0]
            print(f"âœ“ æ£€æµ‹åˆ°ResNetæ¨¡å‹ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–çš„fcå±‚ï¼ˆç±»åˆ«æ•°ï¼š{num_classes}ï¼‰")
            initialization['fc.weight'] = new_initialization['fc.weight']
            initialization['fc.bias'] = new_initialization['fc.bias']
            
            # ResNetè¿˜éœ€è¦æ¢å¤conv1
            if 'conv1.weight' in new_initialization:
                initialization['conv1.weight'] = new_initialization['conv1.weight']
        else:
            raise ValueError("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹ï¼šæ—¢æ²¡æœ‰headä¹Ÿæ²¡æœ‰fcå±‚ï¼è¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")
        
        model.load_state_dict(initialization)
    else:
        # lottery ticket (lt) - ä¹Ÿéœ€è¦å¤„ç†åˆ†ç±»å¤´ä¸åŒ¹é…çš„é—®é¢˜
        print(initialization.keys())
        
        # åˆ¤æ–­æ¨¡å‹ç±»å‹å¹¶å¤„ç†åˆ†ç±»å¤´ï¼ˆäº’æ–¥é€‰æ‹©ï¼‰
        if 'head.weight' in new_initialization:
            # ViTæ¨¡å‹ - æ£€æŸ¥headç»´åº¦æ˜¯å¦åŒ¹é…
            if 'head.weight' in initialization:
                init_classes = initialization['head.weight'].shape[0]
                new_classes = new_initialization['head.weight'].shape[0]
                if init_classes != new_classes:
                    print(f"âš ï¸  æ£€æµ‹åˆ°headç±»åˆ«æ•°ä¸åŒ¹é…ï¼ˆ{init_classes} â†’ {new_classes}ï¼‰ï¼Œä½¿ç”¨æ–°çš„headå±‚")
                    initialization['head.weight'] = new_initialization['head.weight']
                    initialization['head.bias'] = new_initialization['head.bias']
        elif 'fc.weight' in new_initialization:
            
            # ResNetæ¨¡å‹ - æ£€æŸ¥fcç»´åº¦æ˜¯å¦åŒ¹é…
            if 'fc.weight' in initialization:
                init_classes = initialization['fc.weight'].shape[0]
                new_classes = new_initialization['fc.weight'].shape[0]
                if init_classes != new_classes:
                    print(f"âš ï¸  æ£€æµ‹åˆ°fcç±»åˆ«æ•°ä¸åŒ¹é…ï¼ˆ{init_classes} â†’ {new_classes}ï¼‰ï¼Œä½¿ç”¨æ–°çš„fcå±‚")
                    initialization['fc.weight'] = new_initialization['fc.weight']
                    initialization['fc.bias'] = new_initialization['fc.bias']
        
        model.load_state_dict(initialization)
        
    if args.resume:
        print('resume from checkpoint')
        checkpoint = torch.load(args.checkpoint, map_location="cpu")
        try:
            best_sa = checkpoint['best_sa']
            all_result = checkpoint['result']
        except:
            best_sa = checkpoint['best_prec1']
        start_epoch = checkpoint['epoch']
        start_state = checkpoint['state'] if 'state' in checkpoint else 1

        if start_state>0:
            current_mask = extract_mask(checkpoint['state_dict'])
            # for m in current_mask:
            #     print(current_mask[m].float().mean())
            #print(current_mask)
            prune_model_custom(model, current_mask)
            if vit_pruning_utils.is_vit_model(model):
                vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
            else:
                check_sparsity(model, conv1=False)
            optimizer = torch.optim.SGD(model.parameters(), args.lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay)
            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decreasing_lr, gamma=0.1)
        if 'normalize.mean' not in checkpoint['state_dict']:
            checkpoint['state_dict']['normalize.mean'] = new_initialization['normalize.mean']
            checkpoint['state_dict']['normalize.std'] = new_initialization['normalize.std']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        scheduler.load_state_dict(checkpoint['scheduler'])
        initialization = copy.deepcopy(checkpoint['init_weight']) if 'init_weight' in checkpoint else torch.load(args.init)['state_dict']

        if 'normalize.mean' not in initialization:
            initialization['normalize.mean'] = new_initialization['normalize.mean']
            initialization['normalize.std'] = new_initialization['normalize.std']
        print('loading state:', start_state)
        print('loading from epoch: ',start_epoch, 'best_sa=', best_sa)
        all_result = {}
        all_result['train'] = [best_sa]
        all_result['test_ta'] = [best_sa]
        all_result['ta'] = [best_sa]

    else:
        all_result = {}
        all_result['train'] = []
        all_result['test_ta'] = []
        all_result['ta'] = []

        start_epoch = 0
        start_state = 0

    print('######################################## Start Standard Training Iterative Pruning ########################################')


    pruner = None
    for state in range(start_state, args.pruning_times):
        # è®°å½•å½“å‰stateå¼€å§‹æ—¶é—´
        state_start_time = time.time()
        
        # ç›‘è§†æ¨¡å‹

        print('******************************************')
        print('pruning state', state)
        print('******************************************')
        

        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„sparsityæ£€æŸ¥å‡½æ•°
        if vit_pruning_utils.is_vit_model(model):
            remain_weight = vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
        else:
            remain_weight = check_sparsity(model, conv1=False)
        # æ³¨é‡Šæ‰wandb.logä»¥æå‡è®­ç»ƒé€Ÿåº¦
        # wandb.log({'remain_weight': remain_weight})
        if state > 0 and passer.args.struct == 'rsst':
            passer.reg_plot_init = 0

            passer.reg_plot = []
            # æ³¨é‡Šæ‰wandb.logä»¥æå‡è®­ç»ƒé€Ÿåº¦
            # wandb.log({'reg_lambd': passer.reg_plot_init})
            pruner = reg_pruner.Pruner(model, args, passer)  # åˆå§‹åŒ–å‰ªææ„é€ å‡½æ•°
        for epoch in range(start_epoch, args.epochs):

            print(optimizer.state_dict()['param_groups'][0]['lr'])
            if epoch == args.rewind_epoch and args.prune_type == 'rewind_lt' and state == 0:
                torch.save(model.state_dict(), os.path.join(args.save_dir, f"epoch_{args.rewind_epoch}.pth.tar"))
                initialization = copy.deepcopy(model.state_dict())
            if passer.args.struct == 'rsst':
                acc = train(state, train_loader, model, criterion, optimizer, epoch, passer, pruner)
            else:
                acc = train(state, train_loader, model, criterion, optimizer, epoch, passer=None, pruner=None)
            # evaluate on validation set
            tacc = validate(val_loader, model, criterion)
            # evaluate on test set
            test_tacc = validate(test_loader, model, criterion)

            scheduler.step()
            # æ³¨é‡Šæ‰wandb.logä»¥æå‡è®­ç»ƒé€Ÿåº¦
            # è®°å½•è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡ï¼ˆepochçº§åˆ«ï¼Œå‡å°‘ç½‘ç»œè¯·æ±‚é¢‘ç‡ï¼‰
            # wandb.log({
            #     'prune_times': state, 
            #     'epoch': epoch,  
            #     'train_accuracy': acc, 
            #     'val_accuracy': tacc, 
            #     'test_accuracy': test_tacc
            # })

            all_result['train'].append(acc)
            all_result['ta'].append(tacc)
            all_result['test_ta'].append(test_tacc)

            # remember best prec@1 and save checkpoint
            is_best_sa = tacc  > best_sa
            best_sa = max(tacc, best_sa)

            save_checkpoint({
                'state': state,
                'result': all_result,
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'best_sa': best_sa,
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'init_weight': initialization
            }, is_SA_best=is_best_sa, pruning=state, save_path=args.save_dir)
        
            # æ³¨é‡Šæ‰ç»˜å›¾ä»£ç ä»¥æå‡è®­ç»ƒé€Ÿåº¦
            # plt.plot(all_result['train'], label='train_acc')
            # plt.plot(all_result['ta'], label='val_acc')
            # plt.plot(all_result['test_ta'], label='test_acc')
            # plt.legend()
            # plt.savefig(os.path.join(args.save_dir, str(state)+'net_train.png'))
            # plt.close()

        # åœ¨è®­ç»ƒåï¼ˆfinetuningï¼‰å°†æ­£åˆ™åŒ–çš„é¡¹å‰ªæ‰
        if state > 0 and passer.args.struct == 'rsst':
            # passer.reg_plot_init = 0
            passer.reg_plot_dict[state] = passer.reg_plot
            # print('reg_plot_dict',passer.reg_plot_dict)
            # æ³¨é‡Šæ‰ç»˜å›¾ä»£ç ä»¥æå‡è®­ç»ƒé€Ÿåº¦
            # ç»˜åˆ¶æ­£åˆ™åŒ–lambdaæŠ˜çº¿å›¾
            # plt.figure(figsize=(10, 5))  # å¯ä»¥è°ƒæ•´å›¾çš„å¤§å°
            # plt.plot(range(len(passer.reg_plot)), passer.reg_plot, marker='o', linestyle='-', color='b')  # æŠ˜çº¿å›¾ï¼Œå¸¦åœ†å½¢æ ‡è®°

            # æ·»åŠ æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾
            # plt.title("Regularization Parameter Changes Over Iterations")
            # plt.xlabel("Iterations")
            # plt.ylabel("Lambda (Regularization Parameter)")
            # å¯é€‰ï¼šæ·»åŠ ç½‘æ ¼
            # plt.grid(True)
            # ä¿å­˜å›¾è¡¨ï¼ˆæœåŠ¡å™¨ç¯å¢ƒä¸éœ€è¦æ˜¾ç¤ºï¼‰
            # reg_plot_path = os.path.join(args.save_dir, f'reg_plot_state_{state}.png')
            # plt.savefig(reg_plot_path)
            # plt.close()
            # print(f'æ­£åˆ™åŒ–lambdaæŠ˜çº¿å›¾å·²ä¿å­˜åˆ°: {reg_plot_path}')

            # éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œå¹¶ä¸ºæ¯ä¸ªå·ç§¯å±‚/çº¿æ€§å±‚è¿›è¡Œå¤„ç†
            is_vit = vit_pruning_utils.is_vit_model(model)
            for i, (name, m) in enumerate(model.named_modules()):
                # åˆ¤æ–­æ¨¡å—æ˜¯å¦ä¸ºå·ç§¯å±‚ï¼ˆCNNï¼‰æˆ–Linearå±‚ï¼ˆViTï¼‰
                if isinstance(m, nn.Conv2d):
                    # åˆ¤æ–­æ˜¯å¦å¤„ç†ç¬¬ä¸€å±‚å·ç§¯æˆ–è€…å…¶ä»–å·ç§¯å±‚
                    # å¯¹äºViTæ¨¡å‹ï¼Œè·³è¿‡patch_embedå±‚
                    if name != 'conv1' and not (is_vit and 'patch_embed' in name):
                        # è·å–å¯¹åº”çš„æƒé‡æ©ç 
                        mask = passer.current_mask[name + '.weight_mask']
                        # å°†æ©ç å¼ é‡é‡æ–°å¡‘å½¢ä¸ºäºŒç»´ï¼Œå…¶ä¸­ç¬¬ä¸€ç»´æ˜¯é€šé“æ•°
                        mask = mask.view(mask.shape[0], -1)
                        # è®¡ç®—æ¯ä¸ªé€šé“çš„æ©ç ä¹‹å’Œ
                        count = torch.sum(mask, 1)  # æ¯ä¸ªé€šé“çš„1çš„æ•°é‡ï¼Œ[C]

                            # è¿™ä¸€æ­¥åº”è¯¥æ”¾åœ¨æ­£åˆ™åŒ–å å®ç°å‰ªæ
                        m.weight.data = initialization[name + ".weight"]
                        mask = mask.view(*passer.current_mask[name + '.weight_mask'].shape)
                        print('pruning layer with custom mask:', name)
                        prune.CustomFromMask.apply(m, 'weight', mask=mask.to(m.weight.device))
                
                elif isinstance(m, nn.Linear) and is_vit:
                    # å¤„ç†ViTçš„Attentionå’ŒMLPå±‚
                    if 'attn' in name or 'mlp' in name:
                        mask_key = name + '.weight_mask'
                        if mask_key in passer.current_mask:
                            # è¿™ä¸€æ­¥åº”è¯¥æ”¾åœ¨æ­£åˆ™åŒ–å å®ç°å‰ªæ
                            m.weight.data = initialization[name + ".weight"]
                            mask = passer.current_mask[mask_key]
                            print('pruning ViT layer with custom mask:', name)
                        prune.CustomFromMask.apply(m, 'weight', mask=mask.to(m.weight.device))

        #report result
        validate(val_loader, model, criterion) # extra forward
        # check_sparsity(model, conv1=False)

        print('* best SA={}'.format(all_result['test_ta'][np.argmax(np.array(all_result['ta']))]))

        all_result = {}
        all_result['train'] = []
        all_result['test_ta'] = []
        all_result['ta'] = []
        
        # â­ åœ¨éç»“æ„åŒ–å‰ªæä¹‹å‰ä¿å­˜è®­ç»ƒåçš„æƒé‡ï¼ˆå…³é”®ï¼šå¿…é¡»åœ¨pruning_model_vitä¹‹å‰ï¼ï¼‰
        train_weight = model.state_dict()  # ä¿å­˜å®Œæ•´çš„è®­ç»ƒåæƒé‡ï¼Œç”¨äºå‡†ç»“æ„åŒ–å‰ªæçš„é‡è¦æ€§è®¡ç®—
        
        best_sa = 0
        start_epoch = 0

        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å‰ªæå‡½æ•°
        is_vit = vit_pruning_utils.is_vit_model(model)
        
        if is_vit:
            # ========== ViTéç»“æ„åŒ–å‰ªæ (åŸæœ‰é€»è¾‘) ==========
            vit_pruning_utils.pruning_model_vit(model, args.rate, prune_patch_embed=False)
            remain_weight_after = vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
            
            # æå–mask
            current_mask = vit_pruning_utils.extract_mask_vit(model.state_dict())
            passer.current_mask = current_mask
            
            # ç§»é™¤å‰ªæé‡å‚æ•°åŒ–
            vit_pruning_utils.remove_prune_vit(model, prune_patch_embed=False)
            
        else:
            # ========== ResNetå‰ªæ (åŸæœ‰é€»è¾‘) ==========
            pruning_model(model, args.rate, conv1=False)
            remain_weight_after = check_sparsity(model, conv1=False)
            
            # æå–mask
            current_mask = extract_mask(model.state_dict())
            passer.current_mask = current_mask
            
            # ç§»é™¤å‰ªæé‡å‚æ•°åŒ–
            remove_prune(model, conv1=False)
        
        # æ³¨é‡Šæ‰wandb.logä»¥æå‡è®­ç»ƒé€Ÿåº¦
        # if remain_weight_after is not None:
        #     wandb.log({'remain_weight_after': remain_weight_after})

        model.load_state_dict(initialization)
        
        #########################################Refill/RSST Method###########################################################
        if args.struct == 'refill':
            print('æ‰§è¡ŒRefillç®—æ³•')
            if is_vit:
                # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å‡†ç»“æ„åŒ–å‰ªæ
                if args.vit_structured:
                    # ç¡®å®šMLPå‰ªæç‡
                    mlp_ratio = args.mlp_prune_ratio if args.mlp_prune_ratio is not None else args.rate
                    
                    if args.vit_prune_target == 'both':
                        print(f'[ViT] ä½¿ç”¨Head+MLPç»„åˆå‡†ç»“æ„åŒ–å‰ªæ (Refill)')
                        print(f'  - Headå‰ªæç‡: {args.rate}')
                        print(f'  - MLPå‰ªæç‡: {mlp_ratio}')
                        model = vit_pruning_utils_head_mlp.prune_model_custom_fillback_vit_head_and_mlp(
                            model, mask_dict=current_mask, train_loader=train_loader,
                            trained_weight=train_weight, init_weight=initialization,
                            criteria=args.criteria, head_prune_ratio=args.rate, 
                            mlp_prune_ratio=mlp_ratio, return_mask_only=False,
                            sorting_mode=args.sorting_mode)
                    elif args.vit_prune_target == 'head':
                        print('[ViT] ä½¿ç”¨Headçº§åˆ«å‡†ç»“æ„åŒ–å‰ªæ (Refill)')
                        model = vit_pruning_utils.prune_model_custom_fillback_vit_by_head(
                            model, mask_dict=current_mask, train_loader=train_loader,
                            trained_weight=train_weight, init_weight=initialization,
                            criteria=args.criteria, prune_ratio=args.rate,
                            return_mask_only=False)
                    elif args.vit_prune_target == 'mlp':
                        raise NotImplementedError('å•ç‹¬MLPå‰ªæå°šæœªå®ç°ï¼Œè¯·ä½¿ç”¨bothæ¨¡å¼')
                else:
                    print('[ViT] ä½¿ç”¨Element-wiseéç»“æ„åŒ–å‰ªæ (Refill)')
                    model = vit_pruning_utils.prune_model_custom_fillback_vit(
                        model, mask_dict=current_mask, train_loader=train_loader,
                        trained_weight=train_weight, init_weight=initialization,
                        criteria=args.criteria, fillback_rate=args.fillback_rate, 
                        return_mask_only=False)
            else:
                model = prune_model_custom_fillback(
                    model, mask_dict=current_mask, train_loader=train_loader,
                    trained_weight=train_weight, init_weight=initialization,
                    criteria=args.criteria, fillback_rate=args.fillback_rate, 
                    return_mask_only=False)
        elif args.struct == 'rsst':
            print('æ‰§è¡ŒRSSTç®—æ³•')
            # rsstå‰ªæåŠŸèƒ½ è¿”å›refill maskè€Œä¸å‰ªæ
            if is_vit:
                # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å‡†ç»“æ„åŒ–å‰ªæ
                if args.vit_structured:
                    # ç¡®å®šMLPå‰ªæç‡
                    mlp_ratio = args.mlp_prune_ratio if args.mlp_prune_ratio is not None else args.rate
                    
                    if args.vit_prune_target == 'both':
                        print(f'[ViT] ä½¿ç”¨Head+MLPç»„åˆå‡†ç»“æ„åŒ–å‰ªæ (RSST)')
                        print(f'  - Headå‰ªæç‡: {args.rate}')
                        print(f'  - MLPå‰ªæç‡: {mlp_ratio}')
                        mask = vit_pruning_utils_head_mlp.prune_model_custom_fillback_vit_head_and_mlp(
                            model, mask_dict=current_mask, train_loader=train_loader,
                            trained_weight=train_weight, init_weight=initialization,
                            criteria=args.criteria, head_prune_ratio=args.rate,
                            mlp_prune_ratio=mlp_ratio, return_mask_only=True,
                            sorting_mode=args.sorting_mode)
                    elif args.vit_prune_target == 'head':
                        print('[ViT] ä½¿ç”¨Headçº§åˆ«å‡†ç»“æ„åŒ–å‰ªæ (RSST)')
                        mask = vit_pruning_utils.prune_model_custom_fillback_vit_by_head(
                            model, mask_dict=current_mask, train_loader=train_loader,
                            trained_weight=train_weight, init_weight=initialization,
                            criteria=args.criteria, prune_ratio=args.rate,
                            return_mask_only=True)
                    elif args.vit_prune_target == 'mlp':
                        raise NotImplementedError('å•ç‹¬MLPå‰ªæå°šæœªå®ç°ï¼Œè¯·ä½¿ç”¨bothæ¨¡å¼')
                else:
                    print('[ViT] ä½¿ç”¨Element-wiseéç»“æ„åŒ–å‰ªæ (RSST)')
                    mask = vit_pruning_utils.prune_model_custom_fillback_vit(
                        model, mask_dict=current_mask, train_loader=train_loader,
                        trained_weight=train_weight, init_weight=initialization,
                        criteria=args.criteria, fillback_rate=0.0, 
                        return_mask_only=True)
            else:
                mask = prune_model_custom_fillback(
                    model, mask_dict=current_mask, train_loader=train_loader,
                    trained_weight=train_weight, init_weight=initialization,
                    criteria=args.criteria, fillback_rate=0.0, 
                    return_mask_only=True)
            # ä¼ é€’æ­£åˆ™åŒ–ç´¢å¼•
            # passer.current_mask = current_mask
            passer.refill_mask = mask
        else:
            ValueError('é”™è¯¯:æ²¡æœ‰é‚£ä¸ªstructç®—æ³•')

        # æ£€æŸ¥æœ€ç»ˆå‰ªææ•ˆæœ
        if vit_pruning_utils.is_vit_model(model):
            vit_pruning_utils.check_sparsity_vit(model, prune_patch_embed=False)
        else:
            check_sparsity(model, conv1=False)

        optimizer = torch.optim.SGD(model.parameters(), args.lr,
                                    momentum=args.momentum,
                                    weight_decay=args.weight_decay)
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decreasing_lr, gamma=0.1)
        
        # æ‰“å°å½“å‰stateè€—æ—¶
        state_end_time = time.time()
        state_duration = state_end_time - state_start_time
        state_duration_minutes = state_duration / 60.0
        print('=' * 80)
        print(f'â±ï¸  State {state} completed! Time elapsed: {state_duration_minutes:.2f} minutes ({state_duration:.1f} seconds)')
        print('=' * 80)

def update_reg(passer, pruner, model, state, i, j):
    """
    æ›´æ–°æ­£åˆ™åŒ–å‚æ•°
    
    å¯¹äºå‡†ç»“æ„åŒ–å‰ªæï¼ˆåŒ…æ‹¬ViTçš„headçº§åˆ«å‰ªæï¼‰ï¼Œæ­£åˆ™åŒ–ä»ç„¶é€‚ç”¨ï¼š
    - refill_maskæ ‡è®°å“ªäº›weights/headsåº”è¯¥è¢«å‰ªæï¼ˆmask=0ï¼‰
    - æ­£åˆ™åŒ–é€æ¸å‹ç¼©è¿™äº›weightsï¼Œå®ç°æ¸è¿›å¼å‰ªæ
    """
    # å¦‚æœæ²¡æœ‰refill_maskæˆ–current_maskï¼Œè·³è¿‡
    if passer.refill_mask is None or passer.current_mask is None:
        return
    
    is_vit = vit_pruning_utils.is_vit_model(model)

    for name, m in model.named_modules():
        # æ£€æŸ¥æ¨¡å—æ˜¯å¦ä¸ºå·ç§¯å±‚æˆ–çº¿æ€§å±‚
        should_process = False
        if isinstance(m, nn.Conv2d) and not is_vit:
            if name != 'conv1':
                should_process = True
        elif isinstance(m, nn.Linear) and is_vit:
            if 'attn' in name or 'mlp' in name:
                should_process = True
        
        if should_process:
                # æ£€æŸ¥maskæ˜¯å¦å­˜åœ¨
                if name not in passer.refill_mask:
                    continue
                if name + '.weight_mask' not in passer.current_mask:
                    continue
                    
                refill_mask = passer.refill_mask[name].flatten()
                current_mask = passer.current_mask[name + '.weight_mask'].flatten()
                if refill_mask.shape != current_mask.shape:
                    raise ValueError("æ©ç çš„å½¢çŠ¶ä¸åŒ¹é…")
                # è¾“å‡ºéœ€è¦æ­£åˆ™åŒ–çš„é¡¹çš„ç´¢å¼• å½¢çŠ¶ï¼š reg[name][reg_indices[name]] += æ­£åˆ™åŒ–è¡¥å¿
                unpruned_indices = torch.where((refill_mask == 0) & (current_mask == 1))
                unpruned_indices_np = unpruned_indices[0].data.cpu().numpy()
                if passer.args.RST_schedule == 'x':
                    # print('æ›´æ–°æ­£åˆ™åŒ–å‚æ•°lambda')
                    pruner.reg[name][unpruned_indices_np] += passer.args.reg_granularity_prune
                    passer.reg_plot_init += passer.args.reg_granularity_prune
                    passer.reg_plot.append(passer.reg_plot_init)

                if passer.args.RST_schedule == 'x^2':
                    pruner.reg_[name][unpruned_indices_np] += passer.args.reg_granularity_prune
                    pruner.reg[name][unpruned_indices_np] = pruner.reg_[name][unpruned_indices_np] ** 2

                if passer.args.RST_schedule == 'x^3':
                    pruner.reg_[name][unpruned_indices_np] += passer.args.reg_granularity_prune
                    pruner.reg[name][unpruned_indices_np] = pruner.reg_[name][unpruned_indices_np] ** 3
                if passer.args.RST_schedule == 'exp':
                    pruner.reg_[name][unpruned_indices_np] += passer.args.reg_granularity_prune
                    pruner.reg[name][unpruned_indices_np] = torch.tensor(
                        np.exp(pruner.reg_[name][unpruned_indices_np].cpu().numpy()), device=pruner.reg_[name].device)
                    passer.reg_plot_init += passer.args.reg_granularity_prune
                    passer.reg_plot_init = np.exp(passer.reg_plot_init)
                    passer.reg_plot.append(passer.reg_plot_init)
                if passer.args.RST_schedule == 'exp_custom':
                    #print('æ›´æ–°æ­£åˆ™åŒ–å‚æ•°lambda')
                    e = math.exp(1)
                    ceil = 3e-4
                    weight_start = 1 / (e - 1) * passer.args.reg_granularity_prune * (math.exp((i + 1) / j) - 1)
                    pruner.reg[name][unpruned_indices_np] = weight_start

                    passer.reg_plot.append(weight_start)

                if passer.args.RST_schedule == 'exp_custom_exponents':
                    #print('æ›´æ–°æ­£åˆ™åŒ–å‚æ•°lambda')
                    e = math.exp(1)
                    ceil = 3e-4
                    weight_start = 1 / (e - 1) * passer.args.reg_granularity_prune * (math.exp((i + 1) ** args.exponents / j ** args.exponents) - 1)
                    # weight_start = 1 / (e - 1) * passer.args.reg_granularity_prune * (math.exp((i + 1) / j) - 1)
                    pruner.reg[name][unpruned_indices_np] = weight_start

                    passer.reg_plot.append(weight_start)

def train(state,train_loader, model, criterion, optimizer, epoch, passer, pruner):
    
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to train mode
    model.train()

    start = time.time()

    for i, (image, target) in enumerate(train_loader):
        j = len(train_loader)
        if epoch < args.warmup:
            warmup_lr(epoch, i+1, optimizer, one_epoch_step=len(train_loader))

        image = image.cuda()
        target = target.cuda()

        # compute output
        output_clean = model(image)
        loss = criterion(output_clean, target)
        optimizer.zero_grad()
        loss.backward()
        if state > 0  and args.struct == 'rsst':
            # æ›´æ–°æ­£åˆ™åŒ– æ³¨æ„è®¾ç½®æ›´æ–°é—´éš”ä¸æ›´æ–°é˜ˆå€¼
            if passer.args.reg_granularity_prune * i < 1 :
                update_reg(passer, pruner, model, state, i, j)
            #ä¿®æ”¹æ¢¯åº¦ åŠ å…¥regé¡¹
            model = apply_reg(pruner, model, passer)
        optimizer.step()

        output = output_clean.float()
        loss = loss.float()
        # measure accuracy and record loss
        prec1 = accuracy(output.data, target)[0]

        losses.update(loss.item(), image.size(0))
        top1.update(prec1.item(), image.size(0))

        if i % args.print_freq == 0:
            end = time.time()
            print('Epoch: [{0}][{1}/{2}]\t'
                'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                'Accuracy {top1.val:.3f} ({top1.avg:.3f})\t'
                'Time {3:.2f}'.format(
                    epoch, i, len(train_loader), end-start, loss=losses, top1=top1))

            start = time.time()
    # å‡å°‘wandb.logé¢‘ç‡ï¼šåªåœ¨epochç»“æŸæ—¶è®°å½•ï¼Œé¿å…é¢‘ç¹ç½‘ç»œè¯·æ±‚
    # wandb.log({'train_batch_time': end - start, 'loss': losses.avg, 'top1_acc': top1.avg})
    print('train_accuracy {top1.avg:.3f}'.format(top1=top1))

    return top1.avg
def apply_reg(pruner, model, passer):
    # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—
    # print('åº”ç”¨æ­£åˆ™åŒ–åˆ°æ¢¯åº¦')
    for name, m in model.named_modules():
        # æ£€æŸ¥å½“å‰æ¨¡å—åç§°æ˜¯å¦åœ¨æ­£åˆ™åŒ–è§„åˆ™å­—å…¸ä¸­
        if name in pruner.reg:
            # è·å–å½“å‰æ¨¡å—çš„æ­£åˆ™åŒ–æƒé‡
            reg = pruner.reg[name]  # [N, C]
            # print('regnc',reg)
            # å¦‚æœæ˜¯æŒ‰æƒé‡,æ­£åˆ™åŒ–ï¼Œè°ƒæ•´regçš„å½¢çŠ¶ä¸æƒé‡å®Œå…¨ä¸€è‡´
            reg = reg.view_as(m.weight.data)  # [N, C, H, W]
            # print("reg,",torch.unique(reg))
            # reg_lambda_tensor = torch.unique(reg)

            # è®¡ç®—L2æ­£åˆ™åŒ–æ¢¯åº¦
            l2_grad = reg * m.weight

            # æ ¹æ®è®¾ç½®é€‰æ‹©æ˜¯å¦é˜»æ­¢åŸå§‹æ¢¯åº¦çš„åå‘ä¼ æ’­
            if passer.args.block_loss_grad:
                # ä»…ä½¿ç”¨L2æ­£åˆ™åŒ–æ¢¯åº¦æ›´æ–°æƒé‡æ¢¯åº¦
                m.weight.grad = l2_grad
            else:
                # print('å°†L2æ­£åˆ™åŒ–æ¢¯åº¦æ·»åŠ åˆ°åŸæœ‰çš„æƒé‡æ¢¯åº¦ä¸Š')
                m.weight.grad += l2_grad
    # wandb.log({'reg_lambda_tensor': reg_lambda_tensor})
    return model
def validate(val_loader, model, criterion):
    """
    Run evaluation
    """
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to evaluate mode
    model.eval()

    for i, (image, target) in enumerate(val_loader):
        
        image = image.cuda()
        target = target.cuda()

        # compute output
        with torch.no_grad():
            output = model(image)
            loss = criterion(output, target)

        output = output.float()
        loss = loss.float()

        # measure accuracy and record loss
        prec1 = accuracy(output.data, target)[0]
        losses.update(loss.item(), image.size(0))
        top1.update(prec1.item(), image.size(0))

        if i % args.print_freq == 0:
            print('Test: [{0}/{1}]\t'
                'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                'Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(
                    i, len(val_loader), loss=losses, top1=top1))

    print('valid_accuracy {top1.avg:.3f}'
        .format(top1=top1))

    return top1.avg

def save_checkpoint(state, is_SA_best, save_path, pruning, filename='checkpoint.pth.tar'):
    filepath = os.path.join(save_path, str(pruning)+filename)
    torch.save(state, filepath)
    if is_SA_best:
        shutil.copyfile(filepath, os.path.join(save_path, str(pruning)+'model_SA_best.pth.tar'))

def load_weight(model, initialization, args): 
    print('loading pretrained weight')
    loading_weight = extract_main_weight(initialization)
    
    for key in loading_weight.keys():
        if not (key in model.state_dict().keys()):
            print(key)
            assert False

    print('*number of loading weight={}'.format(len(loading_weight.keys())))
    print('*number of model weight={}'.format(len(model.state_dict().keys())))
    model.load_state_dict(loading_weight)

def warmup_lr(epoch, step, optimizer, one_epoch_step):

    overall_steps = args.warmup*one_epoch_step
    current_steps = epoch*one_epoch_step + step 

    lr = args.lr * current_steps/overall_steps
    lr = min(lr, args.lr)

    for p in optimizer.param_groups:
        p['lr']=lr

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def accuracy(output, target, topk=(1,)):
    """Computes the precision@k for the specified values of k"""
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res

def setup_seed(seed): 
    torch.manual_seed(seed) 
    torch.cuda.manual_seed_all(seed) 
    np.random.seed(seed) 
    random.seed(seed) 
    torch.backends.cudnn.deterministic = True 

if __name__ == '__main__':
    main()


